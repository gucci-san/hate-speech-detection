{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT系でベースラインを組む\n",
    "* 実験管理ちゃんとしよう\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** SEED fixed : 42 ******\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, gc, random, time, copy\n",
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import MeCab\n",
    "import re\n",
    "import demoji, mojimoji\n",
    "import neologdn\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    BertJapaneseTokenizer, BertForSequenceClassification, \n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification, \n",
    "    Trainer, TrainingArguments, EvalPrediction, AdamW\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from colorama import Fore\n",
    "b_ = Fore.BLUE; y_ = Fore.YELLOW; g_ = Fore.GREEN; sr_ = Fore.RESET\n",
    "from config import *\n",
    "from myutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTの実装 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length, num_classes, text_col=\"text\"):\n",
    "        self.df = df\n",
    "        self.max_len = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df[text_col].values\n",
    "        self.target = df[label_name].values\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        text = self.text[index]\n",
    "        inputs_text = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        target = int(self.target[index])\n",
    "\n",
    "        onehot_t = np.zeros(self.num_classes, dtype=np.float32)\n",
    "        onehot_t[target] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(inputs_text[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(inputs_text[\"attention_mask\"], dtype=torch.long),\n",
    "            \"target\": torch.tensor(onehot_t, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(HateSpeechModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "            )\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)\n",
    "        out = self.dropout(out[1])\n",
    "        outputs = self.fc(out)\n",
    "        outputs = self.sigmoid(outputs)\n",
    "\n",
    "        return outputs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold, tokenizer, trn_batch_size, val_batch_size, max_length, num_classes, text_col=\"text\"):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = HateSpeechDataset(train_df, tokenizer=tokenizer, max_length=max_length, num_classes=num_classes, text_col=text_col)\n",
    "    valid_dataset = HateSpeechDataset(valid_df, tokenizer=tokenizer, max_length=max_length, num_classes=num_classes, text_col=text_col)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=trn_batch_size, num_workers=2, shuffle=True, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=val_batch_size, num_workers=2, shuffle=False, pin_memory=True\n",
    "    )\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, targets):\n",
    "    loss_f = nn.BCELoss()\n",
    "    return loss_f(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scheduler(scheduler, optimizer, T_max=500, eta_min=1e-7):\n",
    "    if scheduler == \"CosineAnnealingLR\":\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "\n",
    "    else:\n",
    "        print(f\"*** *** NOT implemented *** *** \")\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch, n_accumulate):\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        input_ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        attention_mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"target\"].to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / np.float(n_accumulate)\n",
    "        loss.backward()\n",
    "\n",
    "        if (step+1) % n_accumulate == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        running_loss += (loss.item()*batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    gc.collect()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, optimizer, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        input_ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        attention_mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "        targets = data[\"target\"].to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += (loss.item()*batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    gc.collect()\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, train_loader, valid_loader, optimizer, scheduler, n_accumulate, device, num_epochs, fold, output_path):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"[INFO] Using GPU : {torch.cuda.get_device_name()}\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        gc.collect()\n",
    "\n",
    "        train_epoch_loss = train_one_epoch(\n",
    "            model, optimizer, scheduler,\n",
    "            dataloader=train_loader,\n",
    "            device=device, epoch=epoch,\n",
    "            n_accumulate=n_accumulate\n",
    "        )\n",
    "\n",
    "        valid_epoch_loss = valid_one_epoch(\n",
    "            model, optimizer, \n",
    "            dataloader=valid_loader,\n",
    "            device=device, epoch=epoch,\n",
    "        )\n",
    "\n",
    "        history[\"Train Loss\"].append(train_epoch_loss)\n",
    "        history[\"Valid Loss\"].append(valid_epoch_loss)\n",
    "\n",
    "        if valid_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"{b_}Valid Loss Improved : {best_epoch_loss:.6f} ---> {valid_epoch_loss:.6f}\")\n",
    "            best_epoch_loss = valid_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            #torch.save(model.state_dict(), f\"{output_path}model-state-dict-fold{fold}.bin\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": valid_epoch_loss,\n",
    "            }, f\"{output_path}model-fold{fold}.pth\")  # 途中再開したい場合はmodel.state_dict()以外も必要 --\n",
    "            print(f\"Model Saved{sr_}\"); print()\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_elapsed = end_time - start_time\n",
    "    print(\"Training Complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "        time_elapsed//3600, (time_elapsed%3600)//60, (time_elapsed%3600)%60\n",
    "    ))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メインの実行セル --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"tmp\"\n",
    "output_path = f\"./output/{run_id}/\"\n",
    "\n",
    "epochs = 1\n",
    "folds = 5\n",
    "model_name = r\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "train_batch_size = 32\n",
    "valid_batch_size = 64\n",
    "max_length = 76\n",
    "\n",
    "learning_rate = 1e-6\n",
    "scheduler_name = \"CosineAnnealingLR\"\n",
    "min_lr = 1e-7\n",
    "T_max = 500,\n",
    "weight_decay = 1e-6\n",
    "max_grad_norm = 1.0\n",
    "n_accumulate = 1\n",
    "num_classes = 2\n",
    "n_fold = 5\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 768\n",
    "num_hidden_layers = 24\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80074aa43</td>\n",
       "      <td>news4vip</td>\n",
       "      <td>まともに相手されてない人との関係なんて\\nそんな大事にするものか？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6378fea6b</td>\n",
       "      <td>livejupiter</td>\n",
       "      <td>最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       source                                    text  label\n",
       "0  80074aa43     news4vip       まともに相手されてない人との関係なんて\\nそんな大事にするものか？    0.0\n",
       "1  6378fea6b  livejupiter  最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!    0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\")\n",
    "test = pd.read_csv(data_path+\"test.csv\")\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[:train.shape[0]-1, :]\n",
    "test_df = df.loc[train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=SEED)\n",
    "split = skf.split(train_df, train_df[label_name])\n",
    "\n",
    "for fold, (_, val_index) in enumerate(skf.split(X=train_df, y=train_df[label_name])):\n",
    "    train_df.loc[val_index, \"kfold\"] = int(fold)\n",
    "train_df[\"kfold\"] = train_df[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\",\n",
    "    mecab_kwargs={\"mecab_dic\":None, \"mecab_option\": f\"-d {dic_neologd}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 0 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:15<00:00,  8.69it/s, Epoch=1, LR=8.56e-7, Train_Loss=0.449]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.96it/s, Epoch=1, LR=8.56e-7, Valid_Loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.311017\n",
      "Model Saved\u001b[39m\n",
      "\n",
      "Training Complete in 0h 0m 21s\n",
      "Best Loss: 0.3110\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "for fold in range(0, folds):\n",
    "    print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "\n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=train_batch_size,\n",
    "        val_batch_size=valid_batch_size,\n",
    "        max_length=max_length,\n",
    "        num_classes=num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    # Model construct --\n",
    "    model = HateSpeechModel(model_name=model_name, num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define Optimizer and Scheduler --\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = fetch_scheduler(optimizer=optimizer, scheduler=scheduler_name)\n",
    "\n",
    "    model, history = run_training(\n",
    "        model, train_loader, valid_loader, optimizer, scheduler, n_accumulate, device, epochs, fold, output_path\n",
    "    )\n",
    "\n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "model_paths = glob(f\"{output_path}*.pth\"); model_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output/tmp/model-fold0.pth']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_fn(model, dataloader, device):\n",
    "    model.eval()  # modelはtrainの時点でto(device)されている前提 --\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        input_ids = data[\"input_ids\"].to(device, dtype=torch.long)\n",
    "        attention_mask = data[\"attention_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        preds.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds)\n",
    "    gc.collect()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_name, num_classes, model_paths, dataloader, device):\n",
    "    final_preds = []\n",
    "\n",
    "    for i, path in enumerate([model_paths]):\n",
    "        model = HateSpeechModel(model_name=model_name, num_classes=num_classes)\n",
    "        model.to(device)\n",
    "        checkpoint = torch.load(model_paths)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "        print(f\"Getting predictions for model {i+1}\")\n",
    "        preds = valid_fn(model, dataloader, device)\n",
    "        final_preds.append(preds)\n",
    "\n",
    "\n",
    "    final_preds = np.array(final_preds)\n",
    "    final_preds = np.mean(final_preds, axis=0)\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 0 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "fold_f1 = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(0, folds):\n",
    "    print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "\n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=train_batch_size,\n",
    "        val_batch_size=valid_batch_size,\n",
    "        max_length=max_length,\n",
    "        num_classes=num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    valid = train_df[train_df.kfold == fold]\n",
    "    out = inference(model_name, num_classes, model_paths[fold], valid_loader, device)\n",
    "\n",
    "    valid[\"oof\"] = np.argmax(out, axis=1)\n",
    "\n",
    "    fold_f1.append(f1_score(valid[label_name].values, valid[\"oof\"].values))\n",
    "    fold_acc.append(accuracy_score(valid[label_name].values, valid[\"oof\"].values))\n",
    "\n",
    "    train_df = pd.merge(train_df, valid.loc[:, [\"id\", \"oof\"]], how=\"left\", on=\"id\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=False).to_feather(f\"{output_path}train_df.feather\")\n",
    "test_df.reset_index(drop=False).to_feather(f\"{output_path}test_df.feather\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
