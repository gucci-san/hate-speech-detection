{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch, モデルのパラメータを初期化することについてのnotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(4, 2)\n",
    "        self.f1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.f1(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SampleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 全結合層(nn.Linear)のパラメータ数は\n",
    "    * 係数, 入力数×出力数\n",
    "    * バイアス, 出力数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SampleModel                              --\n",
       "├─Linear: 1-1                            10\n",
       "├─ReLU: 1-2                              --\n",
       "├─Linear: 1-3                            3\n",
       "=================================================================\n",
       "Total params: 13\n",
       "Trainable params: 13\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1290,  0.2064, -0.2678, -0.1653],\n",
      "        [-0.2526, -0.3821,  0.4273, -0.3591]], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([ 0.0835, -0.2762], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[-0.5207,  0.6373]], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.2507], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for params in model.parameters():\n",
    "    print(params, type(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータを学習させたくない場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return y\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss = nn.MSELoss()\n",
    "    return loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = LinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[-0.0902]])),\n",
       "             ('layer.bias', tensor([0.0966]))])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seedしてないから、初期値は毎回変わる --\n",
    "a_before = model.state_dict()[\"layer.weight\"].item()\n",
    "b_before = model.state_dict()[\"layer.bias\"].item()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これでfreezeできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())[0].requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自分で指定した初期値を使いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # これは無理 --\n",
    "# list(model.parameters())[0] = 2\n",
    "# model.state_dict()[\"layer.weight\"] = 3.33333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0093]])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.initを使わないとアカン\n",
    "# freezeしたあとでも差し替え可能 --\n",
    "nn.init.normal_(list(model.parameters())[0], mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[0.0093]])),\n",
       "             ('layer.bias', tensor([0.0966]))])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iiter : 0, loss = 108.496643\n",
      "iiter : 100, loss = 4.338649\n",
      "iiter : 200, loss = 2.257839\n",
      "iiter : 300, loss = 1.533721\n",
      "iiter : 400, loss = 1.879045\n",
      "iiter : 500, loss = 1.950571\n",
      "iiter : 600, loss = 1.232354\n",
      "iiter : 700, loss = 1.624249\n",
      "iiter : 800, loss = 1.659364\n",
      "iiter : 900, loss = 1.304621\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "n = 1000\n",
    "x = torch.rand(n)*2 -1\n",
    "a, b = 2.0, -10.0\n",
    "y = a*x + b\n",
    "\n",
    "x = x + torch.randn(n)*0.02\n",
    "y = y + a*torch.randn(n)*0.02\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "bs = 10\n",
    "niter = 1000\n",
    "losses = []\n",
    "\n",
    "for iiter in range(niter):\n",
    "\n",
    "    r = np.random.choice(n, bs, replace=False)\n",
    "    bx = x[r].reshape(-1, 1)\n",
    "    by = y[r].reshape(-1, 1)\n",
    "\n",
    "    y_ = model(bx)\n",
    "    loss = loss_fn(by, y_)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iiter%100 == 0:\n",
    "       print(f\"iiter : {iiter}, loss = {loss:.6f}\")\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[0.0093]], device='cuda:0')),\n",
       "             ('layer.bias', tensor([-9.9659], device='cuda:0'))])"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_after = model.state_dict()[\"layer.weight\"].item()\n",
    "b_after = model.state_dict()[\"layer.bias\"].item()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_ ... -0.090 -->> 0.009\n",
      "b_ ... 0.097 -->> -9.966\n"
     ]
    }
   ],
   "source": [
    "print(f\"a_ ... {a_before:.3f} -->> {a_after:.3f}\")\n",
    "print(f\"b_ ... {b_before:.3f} -->> {b_after:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多層になった場合、狙ったところをどうやって引っ張るか --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 2)\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TwoLayerModel                            --\n",
       "├─Linear: 1-1                            10\n",
       "├─Linear: 1-2                            3\n",
       "=================================================================\n",
       "Total params: 13\n",
       "Trainable params: 13\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias'])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keysは一次元配置になる --\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l1.weight',\n",
       "              tensor([[-0.3663, -0.3768, -0.0243, -0.2024],\n",
       "                      [ 0.3312,  0.4486,  0.2241, -0.1547]])),\n",
       "             ('l1.bias', tensor([ 0.1602, -0.3296])),\n",
       "             ('l2.weight', tensor([[ 0.4001, -0.3678]])),\n",
       "             ('l2.bias', tensor([-0.3451]))])"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3663, -0.3768, -0.0243, -0.2024],\n",
      "        [ 0.3312,  0.4486,  0.2241, -0.1547]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1602, -0.3296], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4001, -0.3678]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3451], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model_name = \"cl-tohoku/bert-base-japanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m1 = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "BertModel                                          [1, 768]                  --\n",
       "├─BertEmbeddings: 1-1                              [1, 32, 768]              --\n",
       "│    └─Embedding: 2-1                              [1, 32, 768]              24,576,000\n",
       "│    └─Embedding: 2-2                              [1, 32, 768]              1,536\n",
       "│    └─Embedding: 2-3                              [1, 32, 768]              393,216\n",
       "│    └─LayerNorm: 2-4                              [1, 32, 768]              1,536\n",
       "│    └─Dropout: 2-5                                [1, 32, 768]              --\n",
       "├─BertEncoder: 1-2                                 [1, 32, 768]              --\n",
       "│    └─ModuleList: 2-6                             --                        --\n",
       "│    │    └─BertLayer: 3-1                         [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-1                [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-1       [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-1             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-2             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-3             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-4            [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-2          [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-5             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-6            [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-7          [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-2             [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-3                  [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-4          [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-3                         --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-4                   [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-5                  [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-6                 [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-7               [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-4                         [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-5                [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-8       [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-8             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-9             [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-10            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-11           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-9          [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-12            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-13           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-14         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-6             [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-10                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-11         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-6                         --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-8                   [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-12                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-13                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-14              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-7                         [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-9                [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-15      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-15            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-16            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-17            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-18           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-16         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-19            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-20           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-21         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-10            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-17                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-18         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-9                         --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-12                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-19                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-20                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-21              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-10                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-13               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-22      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-22            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-23            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-24            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-25           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-23         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-26            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-27           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-28         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-14            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-24                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-25         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-12                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-16                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-26                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-27                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-28              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-13                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-17               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-29      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-29            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-30            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-31            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-32           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-30         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-33            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-34           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-35         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-18            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-31                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-32         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-15                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-20                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-33                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-34                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-35              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-16                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-21               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-36      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-36            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-37            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-38            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-39           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-37         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-40            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-41           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-42         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-22            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-38                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-39         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-18                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-24                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-40                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-41                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-42              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-19                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-25               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-43      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-43            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-44            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-45            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-46           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-44         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-47            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-48           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-49         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-26            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-45                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-46         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-21                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-28                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-47                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-48                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-49              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-22                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-29               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-50      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-50            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-51            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-52            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-53           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-51         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-54            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-55           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-56         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-30            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-52                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-53         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-24                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-32                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-54                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-55                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-56              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-25                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-33               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-57      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-57            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-58            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-59            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-60           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-58         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-61            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-62           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-63         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-34            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-59                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-60         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-27                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-36                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-61                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-62                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-63              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-28                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-37               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-64      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-64            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-65            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-66            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-67           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-65         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-68            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-69           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-70         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-38            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-66                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-67         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-30                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-40                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-68                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-69                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-70              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-31                        [1, 32, 768]              2,361,600\n",
       "│    │    │    └─BertAttention: 4-41               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-71      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-71            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-72            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-73            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-74           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-72         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-75            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-76           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-77         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-42            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-73                 [1, 32, 3072]             2,362,368\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    │    └─BertIntermediate: 4-43            --                        (recursive)\n",
       "│    │    │    │    └─GELUActivation: 5-74         [1, 32, 3072]             --\n",
       "│    │    └─BertLayer: 3-33                        --                        (recursive)\n",
       "│    │    │    └─BertOutput: 4-44                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-75                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-76                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-77              [1, 32, 768]              1,536\n",
       "│    │    └─BertLayer: 3-34                        [1, 32, 768]              --\n",
       "│    │    │    └─BertAttention: 4-45               [1, 32, 768]              --\n",
       "│    │    │    │    └─BertSelfAttention: 5-78      [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-78            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-79            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Linear: 6-80            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-81           [1, 12, 32, 32]           --\n",
       "│    │    │    │    └─BertSelfOutput: 5-79         [1, 32, 768]              --\n",
       "│    │    │    │    │    └─Linear: 6-82            [1, 32, 768]              590,592\n",
       "│    │    │    │    │    └─Dropout: 6-83           [1, 32, 768]              --\n",
       "│    │    │    │    │    └─LayerNorm: 6-84         [1, 32, 768]              1,536\n",
       "│    │    │    └─BertIntermediate: 4-46            [1, 32, 3072]             --\n",
       "│    │    │    │    └─Linear: 5-80                 [1, 32, 3072]             2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-81         [1, 32, 3072]             --\n",
       "│    │    │    └─BertOutput: 4-47                  [1, 32, 768]              --\n",
       "│    │    │    │    └─Linear: 5-82                 [1, 32, 768]              2,360,064\n",
       "│    │    │    │    └─Dropout: 5-83                [1, 32, 768]              --\n",
       "│    │    │    │    └─LayerNorm: 5-84              [1, 32, 768]              1,536\n",
       "├─BertPooler: 1-3                                  [1, 768]                  --\n",
       "│    └─Linear: 2-7                                 [1, 768]                  590,592\n",
       "│    └─Tanh: 2-8                                   [1, 768]                  --\n",
       "====================================================================================================\n",
       "Total params: 110,617,344\n",
       "Trainable params: 110,617,344\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 110.62\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 26.74\n",
       "Params size (MB): 442.47\n",
       "Estimated Total Size (MB): 469.21\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = (1, 32)\n",
    "dtypes = [torch.int, torch.long]\n",
    "\n",
    "summary(m1, input_size=input_size, dtypes=dtypes, depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m1.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.position_ids', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### そもそもの疑問として、bert-poolerは本当に通ってないのか？という観点がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
