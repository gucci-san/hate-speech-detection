{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "\n",
      "****** SEED fixed : 42 ******\n",
      "\n",
      "\n",
      " \n",
      "@@@@@@@@@@@@@@@ DEBUG with .head(100) @@@@@@@@@@@@@@@@@@\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "217296it [00:00, 1213493.86it/s]\n",
      "1983626it [00:01, 1390968.94it/s]\n",
      "5948218it [00:03, 1508579.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os, time\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bert_utils import *\n",
    "from config import *\n",
    "\n",
    "train_df = pd.read_csv(data_path+\"train.csv\")\n",
    "test_df = pd.read_csv(data_path+\"test.csv\")\n",
    "\n",
    "# debug --\n",
    "train_df = train_df.head(100)\n",
    "test_df = test_df.head(100)\n",
    "print(\"\", \"\")\n",
    "print(\"@@@@@@@@@@@@@@@ DEBUG with .head(100) @@@@@@@@@@@@@@@@@@\")\n",
    "print(\"\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##########################################\n",
    "#\n",
    "#               原文tsbの読み込み\n",
    "#\n",
    "# ##########################################\n",
    "corpus_df_list = []\n",
    "for source in [\"newsplus\", \"news4vip\", \"livejupiter\"]:\n",
    "    corpus_df = pd.DataFrame(columns=[\"id\", \"source\", \"text\"])\n",
    "    tsv_line_list = []\n",
    "    with open(input_root+f\"corpus/{source}.tsv\", encoding=\"utf-8\") as f:\n",
    "        for i, l in tqdm(enumerate(f)):\n",
    "            tsv_line_list.append(l)\n",
    "    corpus_df[\"text\"] = tsv_line_list\n",
    "    corpus_df[\"source\"] = source\n",
    "    corpus_df = corpus_df.reset_index(drop=False)\n",
    "    corpus_df[\"id\"] = corpus_df[\"source\"] + \"_\" + corpus_df[\"index\"].astype(str)\n",
    "    corpus_df = corpus_df.drop([\"index\"], axis=1)\n",
    "    corpus_df_list.append(corpus_df)\n",
    "\n",
    "corpus_df = pd.concat(corpus_df_list)\n",
    "corpus_df = corpus_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def recover_original_text(df, corpus_df):\n",
    "    \n",
    "    original_text_list = []\n",
    "    nofinds_data_index_list = []\n",
    "    duplicated_data_index_list = []\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0]), total=df.shape[0]):\n",
    "        first_sentence = df.loc[i, \"first_sentence\"]\n",
    "        text_picked = [x for x in corpus_df[\"text\"].values.tolist() if first_sentence in x]\n",
    "\n",
    "        if len(text_picked) == 0: # データセットはある程度正規化されているので、データ元からヒットしないケースがある --\n",
    "            nofinds_data_index_list.append(i)\n",
    "            original_text_list.append(\"\")\n",
    "            continue\n",
    "\n",
    "        elif len(text_picked) > 1: # 1つの文に対して2つ以上のレスアンカーがついていた場合, 2つ以上ヒットする --\n",
    "            duplicated_data_index_list.append(i)  # 分離不能な場合があるので諦めてindexだけメモっときます --\n",
    "            original_text_list.append(text_picked[0])\n",
    "\n",
    "        else:  # 1つだけヒット\n",
    "            original_text_list.append(text_picked[0])\n",
    "\n",
    "    df[\"original_text\"] = original_text_list\n",
    "    df = df.drop([\"first_sentence\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def recover_original_text_single(text, corpus_list):\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    return_status, return_text = None, None\n",
    "    text_picked = [x for x in corpus_list if text in x]\n",
    "    \n",
    "    if len(text_picked) == 0: # データセットはある程度正規化されているので、データ元からヒットしないケースがある --\n",
    "        return_status = \"No-result\"\n",
    "        return_text = \"\"\n",
    "\n",
    "    elif len(text_picked) > 1: # 1つの文に対して2つ以上のレスアンカーがついていた場合, 2つ以上ヒットする --\n",
    "        return_status = \"Duplicated\"\n",
    "        return_text = text_picked[0]\n",
    "\n",
    "    else:  # 1つだけヒット\n",
    "        return_status = \"\"\n",
    "        return_text = text_picked[0]\n",
    "\n",
    "    return return_status, return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このケースだとProcessPoolはむしろ遅くなって、ThreadPoolは多分早くなった\n",
    "* そうでもないな..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [02:43<04:08,  3.95s/it]"
     ]
    }
   ],
   "source": [
    "# ########V#################################\n",
    "#\n",
    "#               Trainの原文検索\n",
    "#\n",
    "# ##########################################\n",
    "train_df[\"first_sentence\"] = train_df[\"text\"].map(lambda x: x.split(\"\\n\")[0])\n",
    "with tqdm(total=train_df.shape[0]) as progress:\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = []\n",
    "        for i, d in enumerate(train_df[\"first_sentence\"]):\n",
    "            future = executor.submit(recover_original_text_single, d, corpus_df[\"text\"].values.tolist())\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "        result = [f.result() for f in futures]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.read_feather(\"input/train_df_with_original_text.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "都内の主要幹線道路の一角にいた百円で醤油の効いた海苔巻き餅を売ってくれたオジサン __BR__ あれは旨かった\t磯辺焼きな __BR__ 新宿で買って食ったことあるわ\t見なくなったよな __BR__ こんな話題が2ちゃんで出たのも十年は前だし __BR__ あれpayするのかな？燃料代餅代海苔代醤油代機材償却費…\n",
      "\n",
      "都内の主要幹線道路の一角にいた百円で醤油の効いた海苔巻き餅を売ってくれたオジサン __BR__ あれは旨かった\t磯辺焼きな __BR__ 新宿で買って食ったことあるわ\t見なくなったよな __BR__ こんな話題が2ちゃんで出たのも十年は前だし __BR__ あれpayするのかな？燃料代餅代海苔代醤油代機材償却費…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 9\n",
    "print(result[i][1])\n",
    "print(tt.loc[i, \"original_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
