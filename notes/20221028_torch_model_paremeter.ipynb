{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch, モデルのパラメータを初期化することについてのnotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルモデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleModel, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(4, 2)\n",
    "        self.f1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.f1(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SampleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 全結合層(nn.Linear)のパラメータ数は\n",
    "    * 係数, 入力数×出力数\n",
    "    * バイアス, 出力数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SampleModel                              --\n",
       "├─Linear: 1-1                            10\n",
       "├─ReLU: 1-2                              --\n",
       "├─Linear: 1-3                            3\n",
       "=================================================================\n",
       "Total params: 13\n",
       "Trainable params: 13\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1904, -0.2079,  0.1924,  0.4890],\n",
      "        [ 0.2540,  0.1216, -0.4178,  0.4656]], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0573,  0.2842], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.5925, -0.6490]], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.5286], requires_grad=True) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for params in model.parameters():\n",
    "    print(params, type(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメータを学習させたくない場合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.layer(x)\n",
    "        return y\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    loss = nn.MSELoss()\n",
    "    return loss(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model = LinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[0.9256]])),\n",
       "             ('layer.bias', tensor([0.5275]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seedしてないから、初期値は毎回変わる --\n",
    "a_before = model.state_dict()[\"layer.weight\"].item()\n",
    "b_before = model.state_dict()[\"layer.bias\"].item()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これでfreezeできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(model.parameters())[0].requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### これはfreezeできない, なんでだ？\n",
    "* requires_gradはnn.parameter.Parameterに渡さないと更新されない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()[\"layer.weight\"].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(model.parameters())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自分で指定した初期値を使いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # これは無理 --\n",
    "# list(model.parameters())[0] = 2\n",
    "# model.state_dict()[\"layer.weight\"] = 3.33333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0288]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nn.initを使わないとアカン\n",
    "# freezeしたあとでも差し替え可能 --\n",
    "nn.init.normal_(list(model.parameters())[0], mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dictからアクセスしても行ける。\n",
    "# 要はメモリ渡しだからparamsのtensorに渡せればok\n",
    "nn.init.zeros_(model.state_dict()[\"layer.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[0.]])),\n",
       "             ('layer.bias', tensor([0.5275]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start from  :  a_ = 0.000, b_ = 0.528\n",
      "iiter : 0, loss = 113.505310 / a_ = 0.015, b_ = 0.315\n",
      "iiter : 100, loss = 2.767256 / a_ = 1.260, b_ = -8.666\n",
      "iiter : 200, loss = 0.057084 / a_ = 1.653, b_ = -9.839\n",
      "iiter : 300, loss = 0.011910 / a_ = 1.818, b_ = -9.980\n",
      "iiter : 400, loss = 0.008293 / a_ = 1.909, b_ = -9.999\n",
      "iiter : 500, loss = 0.003688 / a_ = 1.949, b_ = -10.004\n",
      "iiter : 600, loss = 0.003038 / a_ = 1.974, b_ = -10.003\n",
      "iiter : 700, loss = 0.006349 / a_ = 1.984, b_ = -10.001\n",
      "iiter : 800, loss = 0.007312 / a_ = 1.991, b_ = -10.000\n",
      "iiter : 900, loss = 0.000991 / a_ = 1.996, b_ = -10.005\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "n = 1000\n",
    "x = torch.rand(n)*2 -1\n",
    "a, b = 2.0, -10.0\n",
    "y = a*x + b\n",
    "\n",
    "x = x + torch.randn(n)*0.02\n",
    "y = y + a*torch.randn(n)*0.02\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "bs = 10\n",
    "niter = 1000\n",
    "losses = []\n",
    "\n",
    "ta_ = model.state_dict()[\"layer.weight\"].item()\n",
    "tb_ = model.state_dict()[\"layer.bias\"].item()   \n",
    "print(f\"start from  :  a_ = {ta_:.3f}, b_ = {tb_:.3f}\")\n",
    "\n",
    "for iiter in range(niter):\n",
    "\n",
    "   r = np.random.choice(n, bs, replace=False)\n",
    "   bx = x[r].reshape(-1, 1)\n",
    "   by = y[r].reshape(-1, 1)\n",
    "\n",
    "   y_ = model(bx)\n",
    "   loss = loss_fn(by, y_)\n",
    "\n",
    "   optimizer.zero_grad()\n",
    "   loss.backward()\n",
    "   optimizer.step()\n",
    "\n",
    "   ta_ = model.state_dict()[\"layer.weight\"].item()\n",
    "   tb_ = model.state_dict()[\"layer.bias\"].item()   \n",
    "\n",
    "\n",
    "   if iiter%100 == 0:\n",
    "      print(f\"iiter : {iiter}, loss = {loss:.6f} / a_ = {ta_:.3f}, b_ = {tb_:.3f}\")\n",
    "   losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer.weight', tensor([[1.9972]], device='cuda:0')),\n",
       "             ('layer.bias', tensor([-10.0041], device='cuda:0'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_after = model.state_dict()[\"layer.weight\"].item()\n",
    "b_after = model.state_dict()[\"layer.bias\"].item()\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_ ... 0.926 -->> 1.997\n",
      "b_ ... 0.528 -->> -10.004\n"
     ]
    }
   ],
   "source": [
    "print(f\"a_ ... {a_before:.3f} -->> {a_after:.3f}\")\n",
    "print(f\"b_ ... {b_before:.3f} -->> {b_after:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多層になった場合、狙ったところをどうやって引っ張るか --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayerModel, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(4, 2)\n",
    "        self.l2 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TwoLayerModel                            --\n",
       "├─Linear: 1-1                            10\n",
       "├─Linear: 1-2                            3\n",
       "=================================================================\n",
       "Total params: 13\n",
       "Trainable params: 13\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['l1.weight', 'l1.bias', 'l2.weight', 'l2.bias'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keysは一次元配置になる --\n",
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l1.weight',\n",
       "              tensor([[ 0.1994, -0.3865,  0.1245, -0.1823],\n",
       "                      [-0.2441, -0.1220,  0.1161,  0.3535]])),\n",
       "             ('l1.bias', tensor([0.2587, 0.3294])),\n",
       "             ('l2.weight', tensor([[-0.4945, -0.0709]])),\n",
       "             ('l2.bias', tensor([0.1391]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1994, -0.3865,  0.1245, -0.1823],\n",
      "        [-0.2441, -0.1220,  0.1161,  0.3535]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2587, 0.3294], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4945, -0.0709]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1391], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model_name = \"cl-tohoku/bert-base-japanese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m1 = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "BertModel                                          [1, 768]                  --\n",
       "├─BertEmbeddings: 1-1                              [1, 32, 768]              --\n",
       "│    └─Embedding: 2-1                              [1, 32, 768]              24,576,000\n",
       "│    └─Embedding: 2-2                              [1, 32, 768]              1,536\n",
       "│    └─Embedding: 2-3                              [1, 32, 768]              393,216\n",
       "│    └─LayerNorm: 2-4                              [1, 32, 768]              1,536\n",
       "│    └─Dropout: 2-5                                [1, 32, 768]              --\n",
       "├─BertEncoder: 1-2                                 [1, 32, 768]              --\n",
       "│    └─ModuleList: 2-6                             --                        --\n",
       "│    │    └─BertLayer: 3-1                         [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-3                         --                        (recursive)\n",
       "│    │    └─BertLayer: 3-4                         [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-6                         --                        (recursive)\n",
       "│    │    └─BertLayer: 3-7                         [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-9                         --                        (recursive)\n",
       "│    │    └─BertLayer: 3-10                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-12                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-13                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-15                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-16                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-18                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-19                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-21                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-22                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-24                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-25                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-27                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-28                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-30                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-31                        [1, 32, 768]              7,087,872\n",
       "│    │    └─BertLayer: 3-32                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-33                        --                        (recursive)\n",
       "│    │    └─BertLayer: 3-34                        [1, 32, 768]              7,087,872\n",
       "├─BertPooler: 1-3                                  [1, 768]                  --\n",
       "│    └─Linear: 2-7                                 [1, 768]                  590,592\n",
       "│    └─Tanh: 2-8                                   [1, 768]                  --\n",
       "====================================================================================================\n",
       "Total params: 110,617,344\n",
       "Trainable params: 110,617,344\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 110.62\n",
       "====================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 26.74\n",
       "Params size (MB): 442.47\n",
       "Estimated Total Size (MB): 469.21\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = (1, 32)\n",
    "dtypes = [torch.int, torch.long]\n",
    "\n",
    "summary(m1, input_size=input_size, dtypes=dtypes, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m1.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embeddings.position_ids', 'embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### そもそもの疑問として、output_hidden_statesはbert-poolerを本当に通ってないのか？という観点がある\n",
    "* 思ってたとおりの挙動でした\n",
    "    * pooler.dense.weightとpooler.dense.biasが影響するのはbase_output[\"pooler_output\"]のみ\n",
    "    * last_hidden_state -> (pooler) -> pooler_output\n",
    "    * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "model_name = \"rinna/japanese-roberta-base\"\n",
    "input_size = (32, 128)\n",
    "dtypes = [torch.int, torch.long]\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "token = tokenizer.encode_plus(\"例えば君がいるだけで心が強くなれるよ\")\n",
    "input_ids = torch.Tensor(token[\"input_ids\"]).to(torch.long).unsqueeze(0)\n",
    "attention_mask = torch.Tensor(token[\"attention_mask\"]).to(torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/japanese-roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at rinna/japanese-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m2 = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_base_output = m2(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/japanese-roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at rinna/japanese-roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m3 = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pooler_outputっぽいやつのパラメータを初期化 --\n",
    "nn.init.zeros_(m3.state_dict()[\"pooler.dense.weight\"])\n",
    "nn.init.zeros_(m3.state_dict()[\"pooler.dense.bias\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3_base_output = m3(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* まず、m3のpooler_outputは全部ゼロであってほしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok --\n",
    "m3_base_output[\"pooler_output\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* m2のpooler_outputは当然値を持つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7836, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok --\n",
    "m2_base_output[\"pooler_output\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* m3もlast_hidden_stateは値を持つ --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1488,  0.0563, -0.2001,  ..., -0.2476, -0.1276, -0.0199],\n",
       "         [-0.1376,  0.0470, -0.1485,  ..., -0.2500, -0.1300,  0.0111],\n",
       "         [-0.1621,  0.0432, -0.2342,  ..., -0.2326, -0.1819, -0.0077],\n",
       "         ...,\n",
       "         [-0.1684,  0.0294, -0.3133,  ..., -0.1929, -0.0901, -0.0889],\n",
       "         [-0.1868,  0.0360, -0.2739,  ..., -0.2799, -0.0704, -0.0660],\n",
       "         [-0.0763,  0.0404, -0.2715,  ..., -0.2516, -0.0645, -0.0086]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3_base_output[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* で、かつm2_base_outputと同じ値を持っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1488,  0.0563, -0.2001,  ..., -0.2476, -0.1276, -0.0199],\n",
       "         [-0.1376,  0.0470, -0.1485,  ..., -0.2500, -0.1300,  0.0111],\n",
       "         [-0.1621,  0.0432, -0.2342,  ..., -0.2326, -0.1819, -0.0077],\n",
       "         ...,\n",
       "         [-0.1684,  0.0294, -0.3133,  ..., -0.1929, -0.0901, -0.0889],\n",
       "         [-0.1868,  0.0360, -0.2739,  ..., -0.2799, -0.0704, -0.0660],\n",
       "         [-0.0763,  0.0404, -0.2715,  ..., -0.2516, -0.0645, -0.0086]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_base_output[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### では、BERTの狙ったレイヤーだけ初期化する方法はどうする？ --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-large-japanese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m4 = AutoModel.from_pretrained(\"cl-tohoku/bert-large-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BertModel                                          --\n",
       "├─BertEmbeddings: 1-1                              --\n",
       "│    └─Embedding: 2-1                              33,554,432\n",
       "│    └─Embedding: 2-2                              524,288\n",
       "│    └─Embedding: 2-3                              2,048\n",
       "│    └─LayerNorm: 2-4                              2,048\n",
       "│    └─Dropout: 2-5                                --\n",
       "├─BertEncoder: 1-2                                 --\n",
       "│    └─ModuleList: 2-6                             --\n",
       "│    │    └─BertLayer: 3-1                         12,596,224\n",
       "│    │    └─BertLayer: 3-2                         12,596,224\n",
       "│    │    └─BertLayer: 3-3                         12,596,224\n",
       "│    │    └─BertLayer: 3-4                         12,596,224\n",
       "│    │    └─BertLayer: 3-5                         12,596,224\n",
       "│    │    └─BertLayer: 3-6                         12,596,224\n",
       "│    │    └─BertLayer: 3-7                         12,596,224\n",
       "│    │    └─BertLayer: 3-8                         12,596,224\n",
       "│    │    └─BertLayer: 3-9                         12,596,224\n",
       "│    │    └─BertLayer: 3-10                        12,596,224\n",
       "│    │    └─BertLayer: 3-11                        12,596,224\n",
       "│    │    └─BertLayer: 3-12                        12,596,224\n",
       "│    │    └─BertLayer: 3-13                        12,596,224\n",
       "│    │    └─BertLayer: 3-14                        12,596,224\n",
       "│    │    └─BertLayer: 3-15                        12,596,224\n",
       "│    │    └─BertLayer: 3-16                        12,596,224\n",
       "│    │    └─BertLayer: 3-17                        12,596,224\n",
       "│    │    └─BertLayer: 3-18                        12,596,224\n",
       "│    │    └─BertLayer: 3-19                        12,596,224\n",
       "│    │    └─BertLayer: 3-20                        12,596,224\n",
       "│    │    └─BertLayer: 3-21                        12,596,224\n",
       "│    │    └─BertLayer: 3-22                        12,596,224\n",
       "│    │    └─BertLayer: 3-23                        12,596,224\n",
       "│    │    └─BertLayer: 3-24                        12,596,224\n",
       "├─BertPooler: 1-3                                  --\n",
       "│    └─Linear: 2-7                                 1,049,600\n",
       "│    └─Tanh: 2-8                                   --\n",
       "===========================================================================\n",
       "Total params: 337,441,792\n",
       "Trainable params: 337,441,792\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(m4, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.layer.23.attention.self.query.weight',\n",
       " 'encoder.layer.23.attention.self.query.bias',\n",
       " 'encoder.layer.23.attention.self.key.weight',\n",
       " 'encoder.layer.23.attention.self.key.bias',\n",
       " 'encoder.layer.23.attention.self.value.weight',\n",
       " 'encoder.layer.23.attention.self.value.bias',\n",
       " 'encoder.layer.23.attention.output.dense.weight',\n",
       " 'encoder.layer.23.attention.output.dense.bias',\n",
       " 'encoder.layer.23.attention.output.LayerNorm.weight',\n",
       " 'encoder.layer.23.attention.output.LayerNorm.bias',\n",
       " 'encoder.layer.23.intermediate.dense.weight',\n",
       " 'encoder.layer.23.intermediate.dense.bias',\n",
       " 'encoder.layer.23.output.dense.weight',\n",
       " 'encoder.layer.23.output.dense.bias',\n",
       " 'encoder.layer.23.output.LayerNorm.weight',\n",
       " 'encoder.layer.23.output.LayerNorm.bias']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param for param in list(m4.state_dict().keys()) if \"23\" in param]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTでやるとわかりにくいので、Linearでやる --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuadLinearModel, self).__init__()\n",
    "        self.l0 = nn.Linear(1, 1)\n",
    "        self.l1 = nn.Linear(1, 1)\n",
    "        self.l2 = nn.Linear(1, 1)\n",
    "        self.l3 = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l0(x)\n",
    "        out = self.l1(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "QuadLinearModel                          --\n",
       "├─Linear: 1-1                            2\n",
       "├─Linear: 1-2                            2\n",
       "├─Linear: 1-3                            2\n",
       "├─Linear: 1-4                            2\n",
       "=================================================================\n",
       "Total params: 8\n",
       "Trainable params: 8\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QuadLinearModel()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l0.weight', tensor([[-0.6160]])),\n",
       "             ('l0.bias', tensor([-0.8629])),\n",
       "             ('l1.weight', tensor([[-0.3114]])),\n",
       "             ('l1.bias', tensor([0.2078])),\n",
       "             ('l2.weight', tensor([[0.5001]])),\n",
       "             ('l2.bias', tensor([-0.6751])),\n",
       "             ('l3.weight', tensor([[0.5616]])),\n",
       "             ('l3.bias', tensor([-0.8126]))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_init_params_by_name(model, name):\n",
    "    \"\"\"nameを含むnamed_parameterを初期化する関数\"\"\"\n",
    "    init_params = [(param_name, params) for (param_name, params) in model.named_parameters() if name in param_name]\n",
    "    for param in init_params:\n",
    "        print(f\"... {param[0]} initialized ... \")\n",
    "        nn.init.normal_(param[1], mean=0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_freeze_params_by_name(model, name):\n",
    "    \"\"\"nameを含むnamed_parameterをfreeze(required_grad=False)する関数\"\"\"\n",
    "    freeze_params = [(param_name, params) for (param_name, params) in model.named_parameters() if name in param_name]\n",
    "    for param in freeze_params:\n",
    "        print(f\"... {param[0]} freezed ... \")\n",
    "        param[1].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... l1.weight freezed ... \n",
      "... l1.bias freezed ... \n"
     ]
    }
   ],
   "source": [
    "torch_freeze_params_by_name(model, name=\"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l0.weight', tensor([[-0.6160]])),\n",
       "             ('l0.bias', tensor([-0.8629])),\n",
       "             ('l1.weight', tensor([[-0.3114]])),\n",
       "             ('l1.bias', tensor([0.2078])),\n",
       "             ('l2.weight', tensor([[0.5001]])),\n",
       "             ('l2.bias', tensor([-0.6751])),\n",
       "             ('l3.weight', tensor([[0.5616]])),\n",
       "             ('l3.bias', tensor([-0.8126]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "QuadLinearModel                          --\n",
       "├─Linear: 1-1                            2\n",
       "├─Linear: 1-2                            (2)\n",
       "├─Linear: 1-3                            2\n",
       "├─Linear: 1-4                            2\n",
       "=================================================================\n",
       "Total params: 8\n",
       "Trainable params: 6\n",
       "Non-trainable params: 2\n",
       "================================================================="
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT, roBERTaに対して関数を適用してみる --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"nlp-waseda/roberta-large-japanese-seq512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... encoder.layer.23.attention.self.query.weight initialized ... \n",
      "... encoder.layer.23.attention.self.query.bias initialized ... \n",
      "... encoder.layer.23.attention.self.key.weight initialized ... \n",
      "... encoder.layer.23.attention.self.key.bias initialized ... \n",
      "... encoder.layer.23.attention.self.value.weight initialized ... \n",
      "... encoder.layer.23.attention.self.value.bias initialized ... \n",
      "... encoder.layer.23.attention.output.dense.weight initialized ... \n",
      "... encoder.layer.23.attention.output.dense.bias initialized ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.weight initialized ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.bias initialized ... \n",
      "... encoder.layer.23.intermediate.dense.weight initialized ... \n",
      "... encoder.layer.23.intermediate.dense.bias initialized ... \n",
      "... encoder.layer.23.output.dense.weight initialized ... \n",
      "... encoder.layer.23.output.dense.bias initialized ... \n",
      "... encoder.layer.23.output.LayerNorm.weight initialized ... \n",
      "... encoder.layer.23.output.LayerNorm.bias initialized ... \n"
     ]
    }
   ],
   "source": [
    "torch_init_params_by_name(model, name=\"23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... encoder.layer.23.attention.self.query.weight freezed ... \n",
      "... encoder.layer.23.attention.self.query.bias freezed ... \n",
      "... encoder.layer.23.attention.self.key.weight freezed ... \n",
      "... encoder.layer.23.attention.self.key.bias freezed ... \n",
      "... encoder.layer.23.attention.self.value.weight freezed ... \n",
      "... encoder.layer.23.attention.self.value.bias freezed ... \n",
      "... encoder.layer.23.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.23.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.23.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.23.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.23.output.dense.weight freezed ... \n",
      "... encoder.layer.23.output.dense.bias freezed ... \n",
      "... encoder.layer.23.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.23.output.LayerNorm.bias freezed ... \n"
     ]
    }
   ],
   "source": [
    "torch_freeze_params_by_name(model, name=\"23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "RobertaModel                                            --\n",
       "├─RobertaEmbeddings: 1-1                                --\n",
       "│    └─Embedding: 2-1                                   32,768,000\n",
       "│    └─Embedding: 2-2                                   526,336\n",
       "│    └─Embedding: 2-3                                   2,048\n",
       "│    └─LayerNorm: 2-4                                   2,048\n",
       "│    └─Dropout: 2-5                                     --\n",
       "├─RobertaEncoder: 1-2                                   --\n",
       "│    └─ModuleList: 2-6                                  --\n",
       "│    │    └─RobertaLayer: 3-1                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-2                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-3                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-4                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-5                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-6                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-7                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-8                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-9                           12,596,224\n",
       "│    │    └─RobertaLayer: 3-10                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-11                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-12                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-13                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-14                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-15                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-16                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-17                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-18                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-19                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-20                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-21                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-22                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-23                          12,596,224\n",
       "│    │    └─RobertaLayer: 3-24                          (12,596,224)\n",
       "├─RobertaPooler: 1-3                                    --\n",
       "│    └─Linear: 2-7                                      1,049,600\n",
       "│    └─Tanh: 2-8                                        --\n",
       "================================================================================\n",
       "Total params: 336,657,408\n",
       "Trainable params: 324,061,184\n",
       "Non-trainable params: 12,596,224\n",
       "================================================================================"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0041, -0.0408,  0.0207,  ...,  0.0163,  0.0073, -0.0090],\n",
       "        [ 0.0381, -0.0231,  0.0179,  ..., -0.0029, -0.0017,  0.0106],\n",
       "        [-0.0169,  0.0083,  0.0203,  ...,  0.0189,  0.0062, -0.0154],\n",
       "        ...,\n",
       "        [ 0.0091, -0.0253, -0.0038,  ..., -0.0045,  0.0180, -0.0058],\n",
       "        [ 0.0096, -0.0193, -0.0144,  ...,  0.0207, -0.0099,  0.0130],\n",
       "        [ 0.0337,  0.0034,  0.0018,  ...,  0.0003, -0.0154,  0.0076]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()[\"encoder.layer.23.output.dense.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_hidden_layers[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... encoder.layer.18.attention.self.query.weight freezed ... \n",
      "... encoder.layer.18.attention.self.query.bias freezed ... \n",
      "... encoder.layer.18.attention.self.key.weight freezed ... \n",
      "... encoder.layer.18.attention.self.key.bias freezed ... \n",
      "... encoder.layer.18.attention.self.value.weight freezed ... \n",
      "... encoder.layer.18.attention.self.value.bias freezed ... \n",
      "... encoder.layer.18.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.18.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.18.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.18.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.18.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.18.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.18.output.dense.weight freezed ... \n",
      "... encoder.layer.18.output.dense.bias freezed ... \n",
      "... encoder.layer.18.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.18.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.19.attention.self.query.weight freezed ... \n",
      "... encoder.layer.19.attention.self.query.bias freezed ... \n",
      "... encoder.layer.19.attention.self.key.weight freezed ... \n",
      "... encoder.layer.19.attention.self.key.bias freezed ... \n",
      "... encoder.layer.19.attention.self.value.weight freezed ... \n",
      "... encoder.layer.19.attention.self.value.bias freezed ... \n",
      "... encoder.layer.19.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.19.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.19.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.19.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.19.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.19.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.19.output.dense.weight freezed ... \n",
      "... encoder.layer.19.output.dense.bias freezed ... \n",
      "... encoder.layer.19.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.19.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.20.attention.self.query.weight freezed ... \n",
      "... encoder.layer.20.attention.self.query.bias freezed ... \n",
      "... encoder.layer.20.attention.self.key.weight freezed ... \n",
      "... encoder.layer.20.attention.self.key.bias freezed ... \n",
      "... encoder.layer.20.attention.self.value.weight freezed ... \n",
      "... encoder.layer.20.attention.self.value.bias freezed ... \n",
      "... encoder.layer.20.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.20.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.20.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.20.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.20.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.20.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.20.output.dense.weight freezed ... \n",
      "... encoder.layer.20.output.dense.bias freezed ... \n",
      "... encoder.layer.20.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.20.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.21.attention.self.query.weight freezed ... \n",
      "... encoder.layer.21.attention.self.query.bias freezed ... \n",
      "... encoder.layer.21.attention.self.key.weight freezed ... \n",
      "... encoder.layer.21.attention.self.key.bias freezed ... \n",
      "... encoder.layer.21.attention.self.value.weight freezed ... \n",
      "... encoder.layer.21.attention.self.value.bias freezed ... \n",
      "... encoder.layer.21.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.21.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.21.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.21.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.21.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.21.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.21.output.dense.weight freezed ... \n",
      "... encoder.layer.21.output.dense.bias freezed ... \n",
      "... encoder.layer.21.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.21.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.22.attention.self.query.weight freezed ... \n",
      "... encoder.layer.22.attention.self.query.bias freezed ... \n",
      "... encoder.layer.22.attention.self.key.weight freezed ... \n",
      "... encoder.layer.22.attention.self.key.bias freezed ... \n",
      "... encoder.layer.22.attention.self.value.weight freezed ... \n",
      "... encoder.layer.22.attention.self.value.bias freezed ... \n",
      "... encoder.layer.22.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.22.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.22.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.22.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.22.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.22.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.22.output.dense.weight freezed ... \n",
      "... encoder.layer.22.output.dense.bias freezed ... \n",
      "... encoder.layer.22.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.22.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.23.attention.self.query.weight freezed ... \n",
      "... encoder.layer.23.attention.self.query.bias freezed ... \n",
      "... encoder.layer.23.attention.self.key.weight freezed ... \n",
      "... encoder.layer.23.attention.self.key.bias freezed ... \n",
      "... encoder.layer.23.attention.self.value.weight freezed ... \n",
      "... encoder.layer.23.attention.self.value.bias freezed ... \n",
      "... encoder.layer.23.attention.output.dense.weight freezed ... \n",
      "... encoder.layer.23.attention.output.dense.bias freezed ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.23.attention.output.LayerNorm.bias freezed ... \n",
      "... encoder.layer.23.intermediate.dense.weight freezed ... \n",
      "... encoder.layer.23.intermediate.dense.bias freezed ... \n",
      "... encoder.layer.23.output.dense.weight freezed ... \n",
      "... encoder.layer.23.output.dense.bias freezed ... \n",
      "... encoder.layer.23.output.LayerNorm.weight freezed ... \n",
      "... encoder.layer.23.output.LayerNorm.bias freezed ... \n"
     ]
    }
   ],
   "source": [
    "for i in range((model.config.num_hidden_layers-6), (model.config.num_hidden_layers)):\n",
    "    torch_freeze_params_by_name(model, name=f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
