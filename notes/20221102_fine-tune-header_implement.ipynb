{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/02, https://arxiv.org/pdf/1910.12574.pdf, の(d)を実装するnotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "rinna/japanese-roberta-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:213\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/rinna/japanese-roberta-large/resolve/main/spiece.model",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:408\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    409\u001b[0m         path_or_repo_id,\n\u001b[1;32m    410\u001b[0m         filename,\n\u001b[1;32m    411\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    412\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    413\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    414\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    415\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    416\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    417\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    418\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    419\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    422\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1053\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1054\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1055\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1056\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1057\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1058\u001b[0m     )\n\u001b[1;32m   1059\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1060\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1359\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, use_auth_token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1350\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1351\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1352\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1358\u001b[0m )\n\u001b[0;32m-> 1359\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1361\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py:242\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    234\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39melif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Gdlg4sDkln1MMTBlXfLzo)\n\nRepository Not Found for url: https://huggingface.co/rinna/japanese-roberta-large/resolve/main/spiece.model.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [251], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m input_size \u001b[39m=\u001b[39m (\u001b[39m32\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[1;32m      9\u001b[0m dtypes \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mint, torch\u001b[39m.\u001b[39mlong]\n\u001b[0;32m---> 11\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     12\u001b[0m token \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode_plus(\u001b[39m\"\u001b[39m\u001b[39m例えば君がいるだけで心が強くなれるよ。そうはいっても人生楽ありゃ雲あるそ\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(token[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1734\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[1;32m   1733\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1734\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1735\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1736\u001b[0m             file_path,\n\u001b[1;32m   1737\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1738\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1739\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1740\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1741\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1742\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1743\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1744\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1745\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1746\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1747\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1748\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1749\u001b[0m         )\n\u001b[1;32m   1750\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:423\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    408\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    409\u001b[0m         path_or_repo_id,\n\u001b[1;32m    410\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    422\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    424\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n\u001b[1;32m    429\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    430\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: rinna/japanese-roberta-large is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, T5Tokenizer\n",
    "from torchinfo import summary\n",
    "\n",
    "model_name = \"rinna/japanese-roberta-base\"\n",
    "#model_name = \"rinna/japanese-roberta-large\"\n",
    "input_size = (32, 128)\n",
    "dtypes = [torch.int, torch.long]\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "token = tokenizer.encode_plus(\"例えば君がいるだけで心が強くなれるよ。そうはいっても人生楽ありゃ雲あるそ\")\n",
    "input_ids = torch.Tensor(token[\"input_ids\"]).to(torch.long).unsqueeze(0)\n",
    "attention_mask = torch.Tensor(token[\"attention_mask\"]).to(torch.long).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import Single\n",
    "from transformers import AutoConfig\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class BertClassificationMaxPoolingHeader(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(BertClassificationMaxPoolingHeader, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        # max pooling --\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, base_output):\n",
    "        out = base_output[\"hidden_states\"][-1].max(axis=1)[0]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BertClassificationConvolutionHeader(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(BertClassificationConvolutionHeader, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        # conv1d --\n",
    "        self.cnn1 = nn.Conv1d(self.hidden_size, 256, kernel_size=2, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(256, self.num_classes, kernel_size=2, padding=1)\n",
    "\n",
    "    def forward(self, base_output):\n",
    "        last_hidden_state = base_output[\"hidden_states\"][-1].permute(0, 2, 1)\n",
    "        cnn_embeddings = F.relu(self.cnn1(last_hidden_state))\n",
    "        cnn_embeddings = self.cnn2(cnn_embeddings)\n",
    "        outputs = cnn_embeddings.max(axis=2)[0]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertClassificationLSTMHeader(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(BertClassificationLSTMHeader, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        # lstm --\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, base_output):\n",
    "        last_hidden_state = base_output[\"hidden_states\"][-1]\n",
    "        out = self.lstm(last_hidden_state, None)[0]\n",
    "        out = out[:, -1, :]  # lstmの時間方向の最終層を抜く, [batch_size, hidden_size] --\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertClassificationConcatenateHeader(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes, use_layer_num=4):\n",
    "        super(BertClassificationConcatenateHeader, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.use_layer_num = use_layer_num\n",
    "\n",
    "        # concatenate --\n",
    "        self.fc = nn.Linear(self.hidden_size*self.use_layer_num, self.num_classes)\n",
    "        print(self.fc)\n",
    "\n",
    "    def forward(self, base_output):\n",
    "        out = torch.cat([base_output[\"hidden_states\"][-1 * i][:, 0, :] for i in range(1, 4 + 1)], dim=1)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n",
    "\n",
    "class BertClassificationModel(nn.Module):\n",
    "    def __init__(self, model_name, mode=None):\n",
    "        super(BertClassificationModel, self).__init__()\n",
    "        self.cfg = AutoConfig.from_pretrained(\n",
    "            model_name, output_attentions=True, output_hidden_states=True\n",
    "        )\n",
    "        self.l1 = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        if mode==\"max_pooling\":\n",
    "            self.l2 = BertClassificationMaxPoolingHeader(self.cfg.hidden_size, 2)\n",
    "        elif mode==\"conv\":\n",
    "            self.l2 = BertClassificationConvolutionHeader(self.cfg.hidden_size, 2)\n",
    "        elif mode==\"lstm\":\n",
    "            self.l2 = BertClassificationLSTMHeader(self.cfg.hidden_size, 2)\n",
    "        elif mode==\"concatenate\":\n",
    "            self.l2 = BertClassificationConcatenateHeader(self.cfg.hidden_size, 2, use_layer_num=4)\n",
    "        else:\n",
    "            self.l2 = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        hidden_states = self.l1(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "            )\n",
    "        out = self.l2(hidden_states)\n",
    "        return out, hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/japanese-roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at rinna/japanese-roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "m3 = BertClassificationModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, hidden_states = m3(input_ids=input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[\"hidden_states\"][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassificationMozafariHeader(nn.Module):\n",
    "    \"\"\"\n",
    "    Mozafari et al., 2019の(d)を実装したModule\n",
    "    AutoModelのoutput[\"hidden_states\"]を受け取る想定\n",
    "    実装はここを参考(https://github.com/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media/blob/main/Model.py)\n",
    "    \n",
    "    BatchNorm2dの場所は下記を参考\n",
    "    https://qiita.com/cfiken/items/b477c7878828ebdb0387\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, hidden_layer_num, max_length, num_classes):\n",
    "        super(BertClassificationMozafariHeader, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layer_num = hidden_layer_num\n",
    "        self.max_length = max_length\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.hidden_layer_num, out_channels=self.hidden_layer_num, kernel_size=(3, self.hidden_size), padding=1\n",
    "            ) # [batch, hidden_layer, max_length, hidden_size] -> [batch, hidden_layer, max_length, 1]\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=self.hidden_layer_num)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten() # [batch, a, b, c...] -> [batch, a*b*c...]\n",
    "        self.fc = nn.Linear(self.hidden_layer_num*(self.max_length-2), self.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, base_output):\n",
    "        x1 = torch.transpose(\n",
    "            torch.cat(tuple([t.unsqueeze(0) for t in base_output[\"hidden_states\"]][1:]), 0),\n",
    "            0, \n",
    "            1)  # -> [batch, hidden_layer_num, max_length, hidden_size] --\n",
    "        x2 = self.relu(self.batchnorm(self.conv(x1)))\n",
    "        x3 = self.pool(x2)  # \"...the maximum value for each transformer encoder...\"\n",
    "        x4 = self.fc(self.flatten(x3))\n",
    "\n",
    "        return x1, x2, x3, x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output = hidden_states\n",
    "mozafari = BertClassificationMozafariHeader(768, 12, 22, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4 = mozafari(base_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 22, 768])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 22, 3])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 20, 1])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers = 12\n",
    "max_length = 22\n",
    "hidden_size = 768\n",
    "\n",
    "hidden_layers*(max_length-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notebookここまで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 16, 768])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(t[:, -1, :, :], base_output[\"last_hidden_state\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"rinna/japanese-roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 3,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/japanese-roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at rinna/japanese-roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "RobertaModel                                            [32, 768]                 --\n",
       "├─RobertaEmbeddings: 1-1                                [32, 128, 768]            --\n",
       "│    └─Embedding: 2-1                                   [32, 128, 768]            24,576,000\n",
       "│    └─Embedding: 2-2                                   [32, 128, 768]            1,536\n",
       "│    └─Embedding: 2-3                                   [32, 128, 768]            394,752\n",
       "│    └─LayerNorm: 2-4                                   [32, 128, 768]            1,536\n",
       "│    └─Dropout: 2-5                                     [32, 128, 768]            --\n",
       "├─RobertaEncoder: 1-2                                   [32, 128, 768]            --\n",
       "│    └─ModuleList: 2-6                                  --                        --\n",
       "│    │    └─RobertaLayer: 3-1                           [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-1                  [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-2               [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-3                           --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-4                     [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-4                           [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-5                  [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-6               [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-6                           --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-8                     [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-7                           [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-9                  [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-10              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-9                           --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-12                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-10                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-13                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-14              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-12                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-16                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-13                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-17                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-18              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-15                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-20                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-16                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-21                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-22              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-18                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-24                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-19                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-25                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-26              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-21                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-28                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-22                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-29                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-30              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-24                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-32                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-25                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-33                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-34              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-27                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-36                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-28                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-37                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-38              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-30                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-40                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-31                          [32, 128, 768]            2,361,600\n",
       "│    │    │    └─RobertaAttention: 4-41                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-42              [32, 128, 3072]           2,362,368\n",
       "│    │    └─RobertaLayer: 3-32                          --                        (recursive)\n",
       "│    │    │    └─RobertaIntermediate: 4-43              --                        (recursive)\n",
       "│    │    └─RobertaLayer: 3-33                          --                        (recursive)\n",
       "│    │    │    └─RobertaOutput: 4-44                    [32, 128, 768]            2,361,600\n",
       "│    │    └─RobertaLayer: 3-34                          [32, 128, 768]            --\n",
       "│    │    │    └─RobertaAttention: 4-45                 [32, 128, 768]            2,363,904\n",
       "│    │    │    └─RobertaIntermediate: 4-46              [32, 128, 3072]           2,362,368\n",
       "│    │    │    └─RobertaOutput: 4-47                    [32, 128, 768]            2,361,600\n",
       "├─RobertaPooler: 1-3                                    [32, 768]                 --\n",
       "│    └─Linear: 2-7                                      [32, 768]                 590,592\n",
       "│    └─Tanh: 2-8                                        [32, 768]                 --\n",
       "=========================================================================================================\n",
       "Total params: 110,618,880\n",
       "Trainable params: 110,618,880\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.54\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 3422.75\n",
       "Params size (MB): 442.48\n",
       "Estimated Total Size (MB): 3865.24\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(AutoModel.from_pretrained(model_name), input_size=input_size, dtypes=dtypes, depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['l1.embeddings.position_ids', 'l1.embeddings.word_embeddings.weight', 'l1.embeddings.position_embeddings.weight', 'l1.embeddings.token_type_embeddings.weight', 'l1.embeddings.LayerNorm.weight', 'l1.embeddings.LayerNorm.bias', 'l1.encoder.layer.0.attention.self.query.weight', 'l1.encoder.layer.0.attention.self.query.bias', 'l1.encoder.layer.0.attention.self.key.weight', 'l1.encoder.layer.0.attention.self.key.bias', 'l1.encoder.layer.0.attention.self.value.weight', 'l1.encoder.layer.0.attention.self.value.bias', 'l1.encoder.layer.0.attention.output.dense.weight', 'l1.encoder.layer.0.attention.output.dense.bias', 'l1.encoder.layer.0.attention.output.LayerNorm.weight', 'l1.encoder.layer.0.attention.output.LayerNorm.bias', 'l1.encoder.layer.0.intermediate.dense.weight', 'l1.encoder.layer.0.intermediate.dense.bias', 'l1.encoder.layer.0.output.dense.weight', 'l1.encoder.layer.0.output.dense.bias', 'l1.encoder.layer.0.output.LayerNorm.weight', 'l1.encoder.layer.0.output.LayerNorm.bias', 'l1.encoder.layer.1.attention.self.query.weight', 'l1.encoder.layer.1.attention.self.query.bias', 'l1.encoder.layer.1.attention.self.key.weight', 'l1.encoder.layer.1.attention.self.key.bias', 'l1.encoder.layer.1.attention.self.value.weight', 'l1.encoder.layer.1.attention.self.value.bias', 'l1.encoder.layer.1.attention.output.dense.weight', 'l1.encoder.layer.1.attention.output.dense.bias', 'l1.encoder.layer.1.attention.output.LayerNorm.weight', 'l1.encoder.layer.1.attention.output.LayerNorm.bias', 'l1.encoder.layer.1.intermediate.dense.weight', 'l1.encoder.layer.1.intermediate.dense.bias', 'l1.encoder.layer.1.output.dense.weight', 'l1.encoder.layer.1.output.dense.bias', 'l1.encoder.layer.1.output.LayerNorm.weight', 'l1.encoder.layer.1.output.LayerNorm.bias', 'l1.encoder.layer.2.attention.self.query.weight', 'l1.encoder.layer.2.attention.self.query.bias', 'l1.encoder.layer.2.attention.self.key.weight', 'l1.encoder.layer.2.attention.self.key.bias', 'l1.encoder.layer.2.attention.self.value.weight', 'l1.encoder.layer.2.attention.self.value.bias', 'l1.encoder.layer.2.attention.output.dense.weight', 'l1.encoder.layer.2.attention.output.dense.bias', 'l1.encoder.layer.2.attention.output.LayerNorm.weight', 'l1.encoder.layer.2.attention.output.LayerNorm.bias', 'l1.encoder.layer.2.intermediate.dense.weight', 'l1.encoder.layer.2.intermediate.dense.bias', 'l1.encoder.layer.2.output.dense.weight', 'l1.encoder.layer.2.output.dense.bias', 'l1.encoder.layer.2.output.LayerNorm.weight', 'l1.encoder.layer.2.output.LayerNorm.bias', 'l1.encoder.layer.3.attention.self.query.weight', 'l1.encoder.layer.3.attention.self.query.bias', 'l1.encoder.layer.3.attention.self.key.weight', 'l1.encoder.layer.3.attention.self.key.bias', 'l1.encoder.layer.3.attention.self.value.weight', 'l1.encoder.layer.3.attention.self.value.bias', 'l1.encoder.layer.3.attention.output.dense.weight', 'l1.encoder.layer.3.attention.output.dense.bias', 'l1.encoder.layer.3.attention.output.LayerNorm.weight', 'l1.encoder.layer.3.attention.output.LayerNorm.bias', 'l1.encoder.layer.3.intermediate.dense.weight', 'l1.encoder.layer.3.intermediate.dense.bias', 'l1.encoder.layer.3.output.dense.weight', 'l1.encoder.layer.3.output.dense.bias', 'l1.encoder.layer.3.output.LayerNorm.weight', 'l1.encoder.layer.3.output.LayerNorm.bias', 'l1.encoder.layer.4.attention.self.query.weight', 'l1.encoder.layer.4.attention.self.query.bias', 'l1.encoder.layer.4.attention.self.key.weight', 'l1.encoder.layer.4.attention.self.key.bias', 'l1.encoder.layer.4.attention.self.value.weight', 'l1.encoder.layer.4.attention.self.value.bias', 'l1.encoder.layer.4.attention.output.dense.weight', 'l1.encoder.layer.4.attention.output.dense.bias', 'l1.encoder.layer.4.attention.output.LayerNorm.weight', 'l1.encoder.layer.4.attention.output.LayerNorm.bias', 'l1.encoder.layer.4.intermediate.dense.weight', 'l1.encoder.layer.4.intermediate.dense.bias', 'l1.encoder.layer.4.output.dense.weight', 'l1.encoder.layer.4.output.dense.bias', 'l1.encoder.layer.4.output.LayerNorm.weight', 'l1.encoder.layer.4.output.LayerNorm.bias', 'l1.encoder.layer.5.attention.self.query.weight', 'l1.encoder.layer.5.attention.self.query.bias', 'l1.encoder.layer.5.attention.self.key.weight', 'l1.encoder.layer.5.attention.self.key.bias', 'l1.encoder.layer.5.attention.self.value.weight', 'l1.encoder.layer.5.attention.self.value.bias', 'l1.encoder.layer.5.attention.output.dense.weight', 'l1.encoder.layer.5.attention.output.dense.bias', 'l1.encoder.layer.5.attention.output.LayerNorm.weight', 'l1.encoder.layer.5.attention.output.LayerNorm.bias', 'l1.encoder.layer.5.intermediate.dense.weight', 'l1.encoder.layer.5.intermediate.dense.bias', 'l1.encoder.layer.5.output.dense.weight', 'l1.encoder.layer.5.output.dense.bias', 'l1.encoder.layer.5.output.LayerNorm.weight', 'l1.encoder.layer.5.output.LayerNorm.bias', 'l1.encoder.layer.6.attention.self.query.weight', 'l1.encoder.layer.6.attention.self.query.bias', 'l1.encoder.layer.6.attention.self.key.weight', 'l1.encoder.layer.6.attention.self.key.bias', 'l1.encoder.layer.6.attention.self.value.weight', 'l1.encoder.layer.6.attention.self.value.bias', 'l1.encoder.layer.6.attention.output.dense.weight', 'l1.encoder.layer.6.attention.output.dense.bias', 'l1.encoder.layer.6.attention.output.LayerNorm.weight', 'l1.encoder.layer.6.attention.output.LayerNorm.bias', 'l1.encoder.layer.6.intermediate.dense.weight', 'l1.encoder.layer.6.intermediate.dense.bias', 'l1.encoder.layer.6.output.dense.weight', 'l1.encoder.layer.6.output.dense.bias', 'l1.encoder.layer.6.output.LayerNorm.weight', 'l1.encoder.layer.6.output.LayerNorm.bias', 'l1.encoder.layer.7.attention.self.query.weight', 'l1.encoder.layer.7.attention.self.query.bias', 'l1.encoder.layer.7.attention.self.key.weight', 'l1.encoder.layer.7.attention.self.key.bias', 'l1.encoder.layer.7.attention.self.value.weight', 'l1.encoder.layer.7.attention.self.value.bias', 'l1.encoder.layer.7.attention.output.dense.weight', 'l1.encoder.layer.7.attention.output.dense.bias', 'l1.encoder.layer.7.attention.output.LayerNorm.weight', 'l1.encoder.layer.7.attention.output.LayerNorm.bias', 'l1.encoder.layer.7.intermediate.dense.weight', 'l1.encoder.layer.7.intermediate.dense.bias', 'l1.encoder.layer.7.output.dense.weight', 'l1.encoder.layer.7.output.dense.bias', 'l1.encoder.layer.7.output.LayerNorm.weight', 'l1.encoder.layer.7.output.LayerNorm.bias', 'l1.encoder.layer.8.attention.self.query.weight', 'l1.encoder.layer.8.attention.self.query.bias', 'l1.encoder.layer.8.attention.self.key.weight', 'l1.encoder.layer.8.attention.self.key.bias', 'l1.encoder.layer.8.attention.self.value.weight', 'l1.encoder.layer.8.attention.self.value.bias', 'l1.encoder.layer.8.attention.output.dense.weight', 'l1.encoder.layer.8.attention.output.dense.bias', 'l1.encoder.layer.8.attention.output.LayerNorm.weight', 'l1.encoder.layer.8.attention.output.LayerNorm.bias', 'l1.encoder.layer.8.intermediate.dense.weight', 'l1.encoder.layer.8.intermediate.dense.bias', 'l1.encoder.layer.8.output.dense.weight', 'l1.encoder.layer.8.output.dense.bias', 'l1.encoder.layer.8.output.LayerNorm.weight', 'l1.encoder.layer.8.output.LayerNorm.bias', 'l1.encoder.layer.9.attention.self.query.weight', 'l1.encoder.layer.9.attention.self.query.bias', 'l1.encoder.layer.9.attention.self.key.weight', 'l1.encoder.layer.9.attention.self.key.bias', 'l1.encoder.layer.9.attention.self.value.weight', 'l1.encoder.layer.9.attention.self.value.bias', 'l1.encoder.layer.9.attention.output.dense.weight', 'l1.encoder.layer.9.attention.output.dense.bias', 'l1.encoder.layer.9.attention.output.LayerNorm.weight', 'l1.encoder.layer.9.attention.output.LayerNorm.bias', 'l1.encoder.layer.9.intermediate.dense.weight', 'l1.encoder.layer.9.intermediate.dense.bias', 'l1.encoder.layer.9.output.dense.weight', 'l1.encoder.layer.9.output.dense.bias', 'l1.encoder.layer.9.output.LayerNorm.weight', 'l1.encoder.layer.9.output.LayerNorm.bias', 'l1.encoder.layer.10.attention.self.query.weight', 'l1.encoder.layer.10.attention.self.query.bias', 'l1.encoder.layer.10.attention.self.key.weight', 'l1.encoder.layer.10.attention.self.key.bias', 'l1.encoder.layer.10.attention.self.value.weight', 'l1.encoder.layer.10.attention.self.value.bias', 'l1.encoder.layer.10.attention.output.dense.weight', 'l1.encoder.layer.10.attention.output.dense.bias', 'l1.encoder.layer.10.attention.output.LayerNorm.weight', 'l1.encoder.layer.10.attention.output.LayerNorm.bias', 'l1.encoder.layer.10.intermediate.dense.weight', 'l1.encoder.layer.10.intermediate.dense.bias', 'l1.encoder.layer.10.output.dense.weight', 'l1.encoder.layer.10.output.dense.bias', 'l1.encoder.layer.10.output.LayerNorm.weight', 'l1.encoder.layer.10.output.LayerNorm.bias', 'l1.encoder.layer.11.attention.self.query.weight', 'l1.encoder.layer.11.attention.self.query.bias', 'l1.encoder.layer.11.attention.self.key.weight', 'l1.encoder.layer.11.attention.self.key.bias', 'l1.encoder.layer.11.attention.self.value.weight', 'l1.encoder.layer.11.attention.self.value.bias', 'l1.encoder.layer.11.attention.output.dense.weight', 'l1.encoder.layer.11.attention.output.dense.bias', 'l1.encoder.layer.11.attention.output.LayerNorm.weight', 'l1.encoder.layer.11.attention.output.LayerNorm.bias', 'l1.encoder.layer.11.intermediate.dense.weight', 'l1.encoder.layer.11.intermediate.dense.bias', 'l1.encoder.layer.11.output.dense.weight', 'l1.encoder.layer.11.output.dense.bias', 'l1.encoder.layer.11.output.LayerNorm.weight', 'l1.encoder.layer.11.output.LayerNorm.bias', 'l1.pooler.dense.weight', 'l1.pooler.dense.bias'])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5., 4.]]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = nn.MaxPool1d(kernel_size=2, stride=4)\n",
    "\n",
    "tt = torch.Tensor([0, 5, 0, 0, 0, 4]).unsqueeze(0).unsqueeze(0)\n",
    "t(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
