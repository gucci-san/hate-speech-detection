{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original_textで学習回してみるnotebook\n",
    "* いけました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "\n",
      "****** SEED fixed : 256 ******\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AdamW\n",
    "\n",
    "from config import *\n",
    "from bert_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = pd.Series(dict(\n",
    "    folds=5,\n",
    "    model_name=\"cl-tohoku/bert-base-japanese\",\n",
    "    train_batch_size=4,\n",
    "    valid_batch_size=4,\n",
    "    #max_length=128,\n",
    "    max_length=1024,\n",
    "    num_classes=2,\n",
    "    n_accumulate=1,\n",
    "    use_amp=True,\n",
    "    epochs=2,\n",
    "    output_path=\"./output/org_text_tmp/\",\n",
    "    model_custom_header=\"concatenate-4\",\n",
    "    dropout=0.2,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-6,\n",
    "    scheduler_name=\"TEST\"\n",
    "))\n",
    "log = open(settings.output_path + \"/train.log\", \"w\", buffering=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess --\n",
    "train_df = pd.read_feather(\"input/dataset_with_original_text/train_with_original_text.feather\")\n",
    "train_df[\"clean_text\"] = train_df[\"original_text\"].map(lambda x: original_text_preprocess(x))\n",
    "\n",
    "# make folds --\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "split = skf.split(train_df, train_df[label_name])\n",
    "train_df = make_folds(split, train_df, label_name=label_name)\n",
    "\n",
    "# define tokenizer --\n",
    "tokenizer = define_tokenizer(settings.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "*** *** NOT implemented *** *** \n",
      "        --> CosineAnnealingLR *** *** \n",
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1051 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [4, 1024].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m scheduler \u001b[39m=\u001b[39m fetch_scheduler(optimizer\u001b[39m=\u001b[39moptimizer, scheduler\u001b[39m=\u001b[39msettings\u001b[39m.\u001b[39mscheduler_name)\n\u001b[1;32m     27\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m model, history \u001b[39m=\u001b[39m run_training(\n\u001b[1;32m     29\u001b[0m     model, train_loader, valid_loader, \n\u001b[1;32m     30\u001b[0m     optimizer, scheduler, settings\u001b[39m.\u001b[39mn_accumulate, device, settings\u001b[39m.\u001b[39muse_amp, \n\u001b[1;32m     31\u001b[0m     settings\u001b[39m.\u001b[39mepochs, fold, settings\u001b[39m.\u001b[39moutput_path,\n\u001b[1;32m     32\u001b[0m     log, save_checkpoint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[39mdel\u001b[39;00m model, history, train_loader, valid_loader\n\u001b[1;32m     36\u001b[0m _ \u001b[39m=\u001b[39m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/bert_utils.py:616\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(model, train_loader, valid_loader, optimizer, scheduler, n_accumulate, device, use_amp, num_epochs, fold, output_path, log, save_checkpoint, load_checkpoint)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_epoch, num_epochs \u001b[39m+\u001b[39m start_epoch):\n\u001b[1;32m    614\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m--> 616\u001b[0m     train_epoch_loss \u001b[39m=\u001b[39m train_one_epoch(\n\u001b[1;32m    617\u001b[0m         model,\n\u001b[1;32m    618\u001b[0m         optimizer,\n\u001b[1;32m    619\u001b[0m         scheduler,\n\u001b[1;32m    620\u001b[0m         dataloader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m    621\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    622\u001b[0m         use_amp\u001b[39m=\u001b[39;49muse_amp,\n\u001b[1;32m    623\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m    624\u001b[0m         n_accumulate\u001b[39m=\u001b[39;49mn_accumulate,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    627\u001b[0m     valid_epoch_loss \u001b[39m=\u001b[39m valid_one_epoch(\n\u001b[1;32m    628\u001b[0m         model,\n\u001b[1;32m    629\u001b[0m         optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m         epoch\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    635\u001b[0m     history[\u001b[39m\"\u001b[39m\u001b[39mTrain Loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(train_epoch_loss)\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/bert_utils.py:516\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, scheduler, dataloader, device, use_amp, epoch, n_accumulate)\u001b[0m\n\u001b[1;32m    513\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[39mwith\u001b[39;00m autocast(enabled\u001b[39m=\u001b[39muse_amp):\n\u001b[0;32m--> 516\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m    517\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m    518\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mfloat(n_accumulate)\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/bert_utils.py:368\u001b[0m, in \u001b[0;36mHateSpeechModel.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m--> 368\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    369\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    370\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    371\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    372\u001b[0m     )\n\u001b[1;32m    374\u001b[0m     \u001b[39m# 最終的に[batch_size, hidden_size]になるようにcustom_headerを作っていく --\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[39m# https://www.ai-shift.co.jp/techblog/2145 --\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcustom_header \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmax_pooling\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/poetry_projects/hate-speech-detection/.venv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings, \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    987\u001b[0m     buffered_token_type_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings\u001b[39m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 988\u001b[0m     buffered_token_type_ids_expanded \u001b[39m=\u001b[39m buffered_token_type_ids\u001b[39m.\u001b[39;49mexpand(batch_size, seq_length)\n\u001b[1;32m    989\u001b[0m     token_type_ids \u001b[39m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    990\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [4, 1024].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "for fold in range(0, settings.folds):\n",
    "\n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=settings.train_batch_size,\n",
    "        val_batch_size=settings.valid_batch_size,\n",
    "        max_length=settings.max_length,\n",
    "        num_classes=settings.num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    # Model construct --\n",
    "    model = HateSpeechModel(\n",
    "        model_name=settings.model_name,\n",
    "        num_classes=settings.num_classes,\n",
    "        custom_header=settings.model_custom_header,\n",
    "        dropout=settings.dropout,\n",
    "        )\n",
    "\n",
    "    # Define Optimizer and Scheduler --\n",
    "    optimizer = AdamW(model.parameters(), lr=settings.learning_rate, weight_decay=settings.weight_decay)\n",
    "    scheduler = fetch_scheduler(optimizer=optimizer, scheduler=settings.scheduler_name)\n",
    "\n",
    "    model.to(device)\n",
    "    model, history = run_training(\n",
    "        model, train_loader, valid_loader, \n",
    "        optimizer, scheduler, settings.n_accumulate, device, settings.use_amp, \n",
    "        settings.epochs, fold, settings.output_path,\n",
    "        log, save_checkpoint=False\n",
    "    )\n",
    "\n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
