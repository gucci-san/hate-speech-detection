{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コーパスのデータを入力として学習済みモデルから予測結果を出力するnotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "\n",
      "****** SEED fixed : 42 ******\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from glob import glob\n",
    "\n",
    "from bert_utils import *\n",
    "from config import *\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算時のsettingはtrainで保存したjsonから読み込む --\n",
    "# run_idだけ指定 --\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--run_id\", type=str, default=\"roberta_large_cat4\")\n",
    "parser.add_argument(\"--df_start_index\", type=int, default=0)\n",
    "parser.add_argument(\"--df_end_index\", type=int, default=1024)\n",
    "args, unknown = parser.parse_known_args()\n",
    "save_path = f\"{input_root}corpus_label_{args.run_id}\"\n",
    "\n",
    "# コーパス作成時のセッティングを保存 --\n",
    "if not os.path.exists(f\"{save_path}\"):\n",
    "    os.mkdir(f\"{save_path}\")\n",
    "\n",
    "corpus_settings = pd.Series()\n",
    "corpus_settings[\"used_model\"] = args.run_id\n",
    "corpus_settings[\"df_start_index\"] = args.df_start_index\n",
    "corpus_settings[\"df_end_index\"] = args.df_end_index\n",
    "corpus_settings.to_json(save_path+\"/corpus_settings.json\", indent=4)\n",
    "\n",
    "# settings, fine-tuningしたモデル -- \n",
    "output_path = f\"{output_root}{args.run_id}/\"\n",
    "settings = pd.read_json(f\"{output_path}settings.json\", typ=\"series\")\n",
    "model_paths = glob(f\"{settings.output_path}*.pth\"); model_paths.sort()\n",
    "\n",
    "# define tokenizer --\n",
    "tokenizer = define_tokenizer(settings.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m['./input/news4vip.feather', './input/newsplus.feather', './input/livejupiter.feather']\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# 対象とするデータの読み込み --\n",
    "corpus_paths = glob(f\"{input_root}*.feather\")\n",
    "Debug_print(corpus_paths)\n",
    "\n",
    "df = []\n",
    "for corpus_path in corpus_paths:\n",
    "    _df = pd.read_feather(corpus_path)\n",
    "    _df = _df.reset_index(drop=False, names=\"id\")\n",
    "    _df[\"id\"] = corpus_path.split(\"/\")[-1].split(\".\")[0] + \"_\" + _df[\"id\"].astype(str)\n",
    "    df.append(_df)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[args.df_start_index:args.df_end_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test preds --\n",
    "test_dataset = HateSpeechDataset(\n",
    "    df, tokenizer=tokenizer, \n",
    "    max_length=settings.max_length, num_classes=settings.num_classes, \n",
    "    text_col=\"clean_text\", isTrain=False\n",
    "    )\n",
    "\n",
    "# batch_size=512でGPU:19GBくらい --\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "Getting predictions for model : ./output/roberta_large_cat4/checkpoint-fold0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.92s/it]\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "Getting predictions for model : ./output/roberta_large_cat4/checkpoint-fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "Getting predictions for model : ./output/roberta_large_cat4/checkpoint-fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "Getting predictions for model : ./output/roberta_large_cat4/checkpoint-fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.60s/it]\n",
      "Some weights of the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at nlp-waseda/roberta-large-japanese-seq512 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n",
      "Getting predictions for model : ./output/roberta_large_cat4/checkpoint-fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "for fold in range(0, settings.folds):\n",
    "    softmax = nn.Softmax()\n",
    "    model_id = \"model\"\n",
    "    preds = inference(settings.model_name, settings.num_classes, settings.model_custom_header, settings.dropout, model_paths[fold], test_loader, device)\n",
    "    \n",
    "    # preds : BERT -> fc, 確率にするためにsoftmaxに通す必要がある --\n",
    "    preds_list.append(softmax(torch.Tensor(preds)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = np.mean(np.array(preds_list), axis=0)\n",
    "df[f\"{model_id}_pred\"] = np.argmax(final_preds, axis=1)\n",
    "for _class in range(0, settings.num_classes):\n",
    "    df.loc[:, f\"{model_id}_oof_class_{_class}\"] = final_preds[:, _class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f\"{save_path}/corpus_labeled_{args.df_start_index}_to_{args.df_end_index}.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
