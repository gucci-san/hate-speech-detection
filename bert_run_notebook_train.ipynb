{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTのベースラインをスクリプトから実行できるようにリファクタリング --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "\n",
    "from glob import glob\n",
    "from config import *\n",
    "from bert_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"bert-baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = pd.Series(dtype=object)\n",
    "\n",
    "# project settings --\n",
    "settings[\"run_id\"] = RUN_ID\n",
    "settings[\"output_path\"] = f\"{output_root}{settings.run_id}/\"\n",
    "settings[\"num_classes\"] = 2\n",
    "\n",
    "# training settings --\n",
    "settings[\"epochs\"] = 10\n",
    "settings[\"folds\"] = 5\n",
    "settings[\"train_batch_size\"] = 32\n",
    "settings[\"valid_batch_size\"] = 64\n",
    "settings[\"test_batch_size\"] = 64\n",
    "\n",
    "# bert settings --\n",
    "settings[\"model_name\"] = r\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "settings[\"max_length\"] = 76\n",
    "settings[\"hidden_size\"] = 768\n",
    "settings[\"num_hidden_layers\"] = 24\n",
    "settings[\"dropout\"] = 0.2\n",
    "\n",
    "# optimizer settings --\n",
    "settings[\"learning_rate\"] = 1e-5\n",
    "settings[\"scheduler_name\"] = \"CosineAnnealingLR\"\n",
    "settings[\"min_lr\"] = 1e-6\n",
    "settings[\"T_max\"] = 500\n",
    "settings[\"weight_decay\"] = 1e-5\n",
    "settings[\"n_accumulate\"] = 1\n",
    "\n",
    "if not os.path.exists(settings.output_path):\n",
    "    os.mkdir(settings.output_path)\n",
    "\n",
    "os.system(f\"cp ./*py {settings.output_path}\")\n",
    "settings.to_json(f\"{settings.output_path}settings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80074aa43</td>\n",
       "      <td>news4vip</td>\n",
       "      <td>まともに相手されてない人との関係なんて\\nそんな大事にするものか？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6378fea6b</td>\n",
       "      <td>livejupiter</td>\n",
       "      <td>最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       source                                    text  label\n",
       "0  80074aa43     news4vip       まともに相手されてない人との関係なんて\\nそんな大事にするものか？    0.0\n",
       "1  6378fea6b  livejupiter  最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\")\n",
    "test = pd.read_csv(data_path+\"test.csv\")\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "train_shape = train.shape[0]\n",
    "del train, test; _ = gc.collect()\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[:train_shape-1, :]\n",
    "test_df = df.loc[train_shape:, :]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=settings.folds, shuffle=True, random_state=SEED)\n",
    "split = skf.split(train_df, train_df[label_name])\n",
    "\n",
    "for fold, (_, val_index) in enumerate(skf.split(X=train_df, y=train_df[label_name])):\n",
    "    train_df.loc[val_index, \"kfold\"] = int(fold)\n",
    "train_df[\"kfold\"] = train_df[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    settings.model_name,\n",
    "    mecab_kwargs={\"mecab_dic\":None, \"mecab_option\": f\"-d {dic_neologd}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 0 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  9.08it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.251]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.80it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.187037\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  9.12it/s, Epoch=2, LR=4.68e-6, Train_Loss=0.14] \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.83it/s, Epoch=2, LR=4.68e-6, Valid_Loss=0.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.187037 ---> 0.119984\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  9.11it/s, Epoch=3, LR=1.18e-6, Train_Loss=0.0954]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.84it/s, Epoch=3, LR=1.18e-6, Valid_Loss=0.114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.119984 ---> 0.114144\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  9.09it/s, Epoch=4, LR=1.56e-7, Train_Loss=0.0764]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.81it/s, Epoch=4, LR=1.56e-7, Valid_Loss=0.116]\n",
      "100%|██████████| 131/131 [00:14<00:00,  9.05it/s, Epoch=5, LR=2.27e-6, Train_Loss=0.076] \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.70it/s, Epoch=5, LR=2.27e-6, Valid_Loss=0.116]\n",
      "100%|██████████| 131/131 [00:14<00:00,  9.01it/s, Epoch=6, LR=6.16e-6, Train_Loss=0.0812]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.74it/s, Epoch=6, LR=6.16e-6, Valid_Loss=0.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.114144 ---> 0.111831\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  9.04it/s, Epoch=7, LR=9.34e-6, Train_Loss=0.0646]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.80it/s, Epoch=7, LR=9.34e-6, Valid_Loss=0.138]\n",
      "100%|██████████| 131/131 [00:14<00:00,  9.02it/s, Epoch=8, LR=9.78e-6, Train_Loss=0.0521]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.78it/s, Epoch=8, LR=9.78e-6, Valid_Loss=0.119]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.99it/s, Epoch=9, LR=7.19e-6, Train_Loss=0.0203]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.70it/s, Epoch=9, LR=7.19e-6, Valid_Loss=0.169]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.96it/s, Epoch=10, LR=3.23e-6, Train_Loss=0.0147]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.51it/s, Epoch=10, LR=3.23e-6, Valid_Loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete in 0h 2m 60s\n",
      "Best Loss: 0.1118\n",
      "\u001b[33m ====== Fold: 1 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.98it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.249]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.70it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.162127\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.99it/s, Epoch=2, LR=4.68e-6, Train_Loss=0.133]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.71it/s, Epoch=2, LR=4.68e-6, Valid_Loss=0.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.162127 ---> 0.129854\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.96it/s, Epoch=3, LR=1.18e-6, Train_Loss=0.09]  \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.69it/s, Epoch=3, LR=1.18e-6, Valid_Loss=0.134]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.96it/s, Epoch=4, LR=1.56e-7, Train_Loss=0.0752]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.62it/s, Epoch=4, LR=1.56e-7, Valid_Loss=0.132]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.91it/s, Epoch=5, LR=2.27e-6, Train_Loss=0.0726]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.55it/s, Epoch=5, LR=2.27e-6, Valid_Loss=0.137]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=6, LR=6.16e-6, Train_Loss=0.0747]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.58it/s, Epoch=6, LR=6.16e-6, Valid_Loss=0.126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.129854 ---> 0.126271\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.97it/s, Epoch=7, LR=9.34e-6, Train_Loss=0.0622]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.65it/s, Epoch=7, LR=9.34e-6, Valid_Loss=0.167]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.93it/s, Epoch=8, LR=9.78e-6, Train_Loss=0.0407]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.59it/s, Epoch=8, LR=9.78e-6, Valid_Loss=0.17] \n",
      "100%|██████████| 131/131 [00:14<00:00,  8.93it/s, Epoch=9, LR=7.19e-6, Train_Loss=0.0263]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.59it/s, Epoch=9, LR=7.19e-6, Valid_Loss=0.161]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.87it/s, Epoch=10, LR=3.23e-6, Train_Loss=0.0121] \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.50it/s, Epoch=10, LR=3.23e-6, Valid_Loss=0.177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete in 0h 2m 57s\n",
      "Best Loss: 0.1263\n",
      "\u001b[33m ====== Fold: 2 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.87it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.237]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.44it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.159138\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=2, LR=4.68e-6, Train_Loss=0.131]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.80it/s, Epoch=2, LR=4.68e-6, Valid_Loss=0.143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.159138 ---> 0.143228\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.98it/s, Epoch=3, LR=1.18e-6, Train_Loss=0.0822]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.82it/s, Epoch=3, LR=1.18e-6, Valid_Loss=0.131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.143228 ---> 0.130952\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.99it/s, Epoch=4, LR=1.56e-7, Train_Loss=0.0614]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.83it/s, Epoch=4, LR=1.56e-7, Valid_Loss=0.133]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.93it/s, Epoch=5, LR=2.27e-6, Train_Loss=0.0595]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.62it/s, Epoch=5, LR=2.27e-6, Valid_Loss=0.14] \n",
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=6, LR=6.16e-6, Train_Loss=0.0557]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.50it/s, Epoch=6, LR=6.16e-6, Valid_Loss=0.142]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.89it/s, Epoch=7, LR=9.34e-6, Train_Loss=0.0462]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.49it/s, Epoch=7, LR=9.34e-6, Valid_Loss=0.158]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.88it/s, Epoch=8, LR=9.78e-6, Train_Loss=0.0326]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.45it/s, Epoch=8, LR=9.78e-6, Valid_Loss=0.197]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.86it/s, Epoch=9, LR=7.19e-6, Train_Loss=0.0294]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.45it/s, Epoch=9, LR=7.19e-6, Valid_Loss=0.155]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.82it/s, Epoch=10, LR=3.23e-6, Train_Loss=0.0141]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.49it/s, Epoch=10, LR=3.23e-6, Valid_Loss=0.175]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete in 0h 2m 57s\n",
      "Best Loss: 0.1310\n",
      "\u001b[33m ====== Fold: 3 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.89it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.248]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.59it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.180813\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.91it/s, Epoch=2, LR=4.68e-6, Train_Loss=0.142]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.59it/s, Epoch=2, LR=4.68e-6, Valid_Loss=0.125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.180813 ---> 0.124661\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=3, LR=1.18e-6, Train_Loss=0.0953]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.37it/s, Epoch=3, LR=1.18e-6, Valid_Loss=0.123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.124661 ---> 0.122527\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.91it/s, Epoch=4, LR=1.56e-7, Train_Loss=0.0837]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.44it/s, Epoch=4, LR=1.56e-7, Valid_Loss=0.123]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.86it/s, Epoch=5, LR=2.27e-6, Train_Loss=0.0795]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.44it/s, Epoch=5, LR=2.27e-6, Valid_Loss=0.122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.122527 ---> 0.122290\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.89it/s, Epoch=6, LR=6.16e-6, Train_Loss=0.081] \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.54it/s, Epoch=6, LR=6.16e-6, Valid_Loss=0.137]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=7, LR=9.34e-6, Train_Loss=0.0716]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.51it/s, Epoch=7, LR=9.34e-6, Valid_Loss=0.125]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.86it/s, Epoch=8, LR=9.78e-6, Train_Loss=0.0463]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.44it/s, Epoch=8, LR=9.78e-6, Valid_Loss=0.143]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.81it/s, Epoch=9, LR=7.19e-6, Train_Loss=0.0231]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.46it/s, Epoch=9, LR=7.19e-6, Valid_Loss=0.168]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.79it/s, Epoch=10, LR=3.23e-6, Train_Loss=0.0134]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.46it/s, Epoch=10, LR=3.23e-6, Valid_Loss=0.187]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete in 0h 3m 2s\n",
      "Best Loss: 0.1223\n",
      "\u001b[33m ====== Fold: 4 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.84it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.256]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.38it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : inf ---> 0.152967\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.85it/s, Epoch=2, LR=4.68e-6, Train_Loss=0.14] \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.63it/s, Epoch=2, LR=4.68e-6, Valid_Loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.152967 ---> 0.109253\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.99it/s, Epoch=3, LR=1.18e-6, Train_Loss=0.0936]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.51it/s, Epoch=3, LR=1.18e-6, Valid_Loss=0.109] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.109253 ---> 0.108724\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.93it/s, Epoch=4, LR=1.56e-7, Train_Loss=0.0727]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.57it/s, Epoch=4, LR=1.56e-7, Valid_Loss=0.112]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.99it/s, Epoch=5, LR=2.27e-6, Train_Loss=0.0712]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.71it/s, Epoch=5, LR=2.27e-6, Valid_Loss=0.117]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.98it/s, Epoch=6, LR=6.16e-6, Train_Loss=0.0668]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.71it/s, Epoch=6, LR=6.16e-6, Valid_Loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mValid Loss Improved : 0.108724 ---> 0.107844\n",
      "Model Saved\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.96it/s, Epoch=7, LR=9.34e-6, Train_Loss=0.0571]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.57it/s, Epoch=7, LR=9.34e-6, Valid_Loss=0.21] \n",
      "100%|██████████| 131/131 [00:14<00:00,  8.95it/s, Epoch=8, LR=9.78e-6, Train_Loss=0.0379]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.65it/s, Epoch=8, LR=9.78e-6, Valid_Loss=0.142]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.94it/s, Epoch=9, LR=7.19e-6, Train_Loss=0.02]  \n",
      "100%|██████████| 17/17 [00:01<00:00, 14.63it/s, Epoch=9, LR=7.19e-6, Valid_Loss=0.154]\n",
      "100%|██████████| 131/131 [00:14<00:00,  8.93it/s, Epoch=10, LR=3.23e-6, Train_Loss=0.0142]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.42it/s, Epoch=10, LR=3.23e-6, Valid_Loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete in 0h 3m 2s\n",
      "Best Loss: 0.1078\n"
     ]
    }
   ],
   "source": [
    "for fold in range(0, settings.folds):\n",
    "    print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "\n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=settings.train_batch_size,\n",
    "        val_batch_size=settings.valid_batch_size,\n",
    "        max_length=settings.max_length,\n",
    "        num_classes=settings.num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    # Model construct --\n",
    "    model = HateSpeechModel(model_name=settings.model_name, num_classes=settings.num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define Optimizer and Scheduler --\n",
    "    optimizer = AdamW(model.parameters(), lr=settings.learning_rate, weight_decay=settings.weight_decay)\n",
    "    scheduler = fetch_scheduler(optimizer=optimizer, scheduler=settings.scheduler_name)\n",
    "\n",
    "    model, history = run_training(\n",
    "        model, train_loader, valid_loader, optimizer, scheduler, settings.n_accumulate, device, settings.epochs, fold, settings.output_path\n",
    "    )\n",
    "\n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output/bert-baseline/model-fold0.pth',\n",
       " './output/bert-baseline/model-fold1.pth',\n",
       " './output/bert-baseline/model-fold2.pth',\n",
       " './output/bert-baseline/model-fold3.pth',\n",
       " './output/bert-baseline/model-fold4.pth']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = glob(f\"{settings.output_path}*.pth\"); model_paths.sort()\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 0 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/bert-baseline/model-fold0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 1 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/bert-baseline/model-fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 2 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/bert-baseline/model-fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 3 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/bert-baseline/model-fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 4 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/bert-baseline/model-fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.90it/s]\n"
     ]
    }
   ],
   "source": [
    "fold_f1 = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(0, settings.folds):\n",
    "    print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "\n",
    "    model_id = model_paths[fold].split(\"/\")[3].split(\".\")[0].split(\"-\")[0]\n",
    "    \n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=settings.train_batch_size,\n",
    "        val_batch_size=settings.valid_batch_size,\n",
    "        max_length=settings.max_length,\n",
    "        num_classes=settings.num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    valid = train_df[train_df.kfold == fold]\n",
    "    out = inference(settings.model_name, settings.num_classes, model_paths[fold], valid_loader, device)\n",
    "\n",
    "    for _class in range(0, settings.num_classes):\n",
    "        valid[f\"{model_id}_oof_class{_class}\"] = out[:, _class]\n",
    "        train_df.loc[valid.index.tolist(), f\"{model_id}_oof_class_{_class}\"] = valid[f\"{model_id}_oof_class{_class}\"]\n",
    "\n",
    "    valid_preds = np.argmax(out, axis=1)\n",
    "\n",
    "    fold_f1.append(f1_score(valid[label_name].values, valid_preds))\n",
    "    fold_acc.append(accuracy_score(valid[label_name].values, valid_preds))\n",
    "\n",
    "    train_df.loc[valid.index.tolist(), f\"{model_id}_pred\"] = valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=False).to_feather(f\"{settings.output_path}train_df.feather\")\n",
    "test_df.reset_index(drop=False).to_feather(f\"{settings.output_path}test_df.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m mean_valid_metric : f1 = 0.6054 ... acc = 0.9545\n",
      "\u001b[32m  all_valid_metric : f1 = 0.6063 ... acc = 0.9545 \n"
     ]
    }
   ],
   "source": [
    "print(f\"{g_} mean_valid_metric : f1 = {np.mean(fold_f1):.4f} ... acc = {np.mean(fold_acc):.4f}\")\n",
    "print(f\"{g_}  all_valid_metric : f1 = {f1_score(train_df.label, train_df.model_pred):.4f} ... acc = {accuracy_score(train_df.label, train_df.model_pred):.4f} \")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
