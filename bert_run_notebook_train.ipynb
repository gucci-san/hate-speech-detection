{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTのベースラインをスクリプトから実行できるようにリファクタリング --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "\n",
    "from glob import glob\n",
    "from config import *\n",
    "from bert_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"implement-exp-manage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = pd.Series(dtype=object)\n",
    "\n",
    "# project settings --\n",
    "settings[\"run_id\"] = RUN_ID\n",
    "settings[\"output_path\"] = f\"{output_root}{settings.run_id}/\"\n",
    "settings[\"num_classes\"] = 2\n",
    "\n",
    "# training settings --\n",
    "settings[\"epochs\"] = 1\n",
    "settings[\"folds\"] = 5\n",
    "settings[\"train_batch_size\"] = 32\n",
    "settings[\"valid_batch_size\"] = 64\n",
    "settings[\"test_batch_size\"] = 64\n",
    "\n",
    "# bert settings --\n",
    "settings[\"model_name\"] = r\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
    "settings[\"max_length\"] = 76\n",
    "settings[\"hidden_size\"] = 768\n",
    "settings[\"num_hidden_layers\"] = 24\n",
    "settings[\"dropout\"] = 0.2\n",
    "\n",
    "# optimizer settings --\n",
    "settings[\"learning_rate\"] = 1e-5\n",
    "settings[\"scheduler_name\"] = \"CosineAnnealingLR\"\n",
    "settings[\"min_lr\"] = 1e-6\n",
    "settings[\"T_max\"] = 500\n",
    "settings[\"weight_decay\"] = 1e-5\n",
    "settings[\"n_accumulate\"] = 1\n",
    "\n",
    "if not os.path.exists(settings.output_path):\n",
    "    os.mkdir(settings.output_path)\n",
    "\n",
    "os.system(f\"cp ./*py {settings.output_path}\")\n",
    "settings.to_json(f\"{settings.output_path}settings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80074aa43</td>\n",
       "      <td>news4vip</td>\n",
       "      <td>まともに相手されてない人との関係なんて\\nそんな大事にするものか？</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6378fea6b</td>\n",
       "      <td>livejupiter</td>\n",
       "      <td>最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       source                                    text  label\n",
       "0  80074aa43     news4vip       まともに相手されてない人との関係なんて\\nそんな大事にするものか？    0.0\n",
       "1  6378fea6b  livejupiter  最近はアヘアヘQSマンやない？ ｲｲ!(・∀・)+1-0(・Ａ・)ｲｸﾅｲ!    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv(data_path+\"train.csv\")\n",
    "test = pd.read_csv(data_path+\"test.csv\")\n",
    "\n",
    "df = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "train_shape = train.shape[0]\n",
    "del train, test; _ = gc.collect()\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[:train_shape-1, :]\n",
    "test_df = df.loc[train_shape:, :]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=settings.folds, shuffle=True, random_state=SEED)\n",
    "split = skf.split(train_df, train_df[label_name])\n",
    "\n",
    "for fold, (_, val_index) in enumerate(skf.split(X=train_df, y=train_df[label_name])):\n",
    "    train_df.loc[val_index, \"kfold\"] = int(fold)\n",
    "train_df[\"kfold\"] = train_df[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    settings.model_name,\n",
    "    mecab_kwargs={\"mecab_dic\":None, \"mecab_option\": f\"-d {dic_neologd}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************** TRAINING ********************\n",
      "\n",
      "================== Fold: 0 ==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.95it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.245]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.65it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.181440\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 20s\n",
      "Best Loss: 0.1814\n",
      "\n",
      "================== Fold: 1 ==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.90it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.243]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.59it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.184069\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 22s\n",
      "Best Loss: 0.1841\n",
      "\n",
      "================== Fold: 2 ==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.88it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.237]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.57it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.192670\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 22s\n",
      "Best Loss: 0.1927\n",
      "\n",
      "================== Fold: 3 ==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.88it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.234]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.77it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.162024\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 17s\n",
      "Best Loss: 0.1620\n",
      "\n",
      "================== Fold: 4 ==================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:14<00:00,  8.87it/s, Epoch=1, LR=8.42e-6, Train_Loss=0.246]\n",
      "100%|██████████| 17/17 [00:01<00:00, 14.60it/s, Epoch=1, LR=8.42e-6, Valid_Loss=0.163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.162869\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 17s\n",
      "Best Loss: 0.1629\n"
     ]
    }
   ],
   "source": [
    "log = open(settings.output_path + \"/train.log\", \"w\", buffering=1)\n",
    "Write_log(log, \"***************** TRAINING ********************\")\n",
    "\n",
    "for fold in range(0, settings.folds):\n",
    "    \n",
    "    #print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "    Write_log(log, f\"\\n================== Fold: {fold} ==================\")\n",
    "\n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=settings.train_batch_size,\n",
    "        val_batch_size=settings.valid_batch_size,\n",
    "        max_length=settings.max_length,\n",
    "        num_classes=settings.num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    # Model construct --\n",
    "    model = HateSpeechModel(model_name=settings.model_name, num_classes=settings.num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define Optimizer and Scheduler --\n",
    "    optimizer = AdamW(model.parameters(), lr=settings.learning_rate, weight_decay=settings.weight_decay)\n",
    "    scheduler = fetch_scheduler(optimizer=optimizer, scheduler=settings.scheduler_name)\n",
    "\n",
    "    model, history = run_training(\n",
    "        model, train_loader, valid_loader, optimizer, scheduler, settings.n_accumulate, device, settings.epochs, fold, settings.output_path, log\n",
    "    )\n",
    "\n",
    "    del model, history, train_loader, valid_loader\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./output/implement-exp-manage/model-fold0.pth',\n",
       " './output/implement-exp-manage/model-fold1.pth',\n",
       " './output/implement-exp-manage/model-fold2.pth',\n",
       " './output/implement-exp-manage/model-fold3.pth',\n",
       " './output/implement-exp-manage/model-fold4.pth']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths = glob(f\"{settings.output_path}*.pth\"); model_paths.sort()\n",
    "model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 0 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/implement-exp-manage/model-fold0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 1 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/implement-exp-manage/model-fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 2 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/implement-exp-manage/model-fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 3 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/implement-exp-manage/model-fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m ====== Fold: 4 ======\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions for model : ./output/implement-exp-manage/model-fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.80it/s]\n"
     ]
    }
   ],
   "source": [
    "fold_f1 = []\n",
    "fold_acc = []\n",
    "\n",
    "for fold in range(0, settings.folds):\n",
    "    print(f\"{y_} ====== Fold: {fold} ======{sr_}\")\n",
    "\n",
    "    model_id = model_paths[fold].split(\"/\")[3].split(\".\")[0].split(\"-\")[0]\n",
    "    \n",
    "    # Create DataLoader --\n",
    "    train_loader, valid_loader = prepare_loaders(\n",
    "        df=train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        fold=fold,\n",
    "        trn_batch_size=settings.train_batch_size,\n",
    "        val_batch_size=settings.valid_batch_size,\n",
    "        max_length=settings.max_length,\n",
    "        num_classes=settings.num_classes,\n",
    "        text_col=\"clean_text\"\n",
    "    )\n",
    "\n",
    "    valid = train_df[train_df.kfold == fold]\n",
    "    out = inference(settings.model_name, settings.num_classes, model_paths[fold], valid_loader, device)\n",
    "\n",
    "    for _class in range(0, settings.num_classes):\n",
    "        valid[f\"{model_id}_oof_class{_class}\"] = out[:, _class]\n",
    "        train_df.loc[valid.index.tolist(), f\"{model_id}_oof_class_{_class}\"] = valid[f\"{model_id}_oof_class{_class}\"]\n",
    "\n",
    "    valid_preds = np.argmax(out, axis=1)\n",
    "\n",
    "    fold_f1.append(f1_score(valid[label_name].values, valid_preds))\n",
    "    fold_acc.append(accuracy_score(valid[label_name].values, valid_preds))\n",
    "\n",
    "    train_df.loc[valid.index.tolist(), f\"{model_id}_pred\"] = valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=False).to_feather(f\"{settings.output_path}train_df.feather\")\n",
    "test_df.reset_index(drop=False).to_feather(f\"{settings.output_path}test_df.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      ">> mean_valid_metric : f1 = 0.0000 ... acc = 0.9418\n",
      ">>  all_valid_metric : f1 = 0.0000 ... acc = 0.9418 \n"
     ]
    }
   ],
   "source": [
    "Write_log(log, \"\\n++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "\n",
    "Write_log(log, f\">> mean_valid_metric : f1 = {np.mean(fold_f1):.4f} ... acc = {np.mean(fold_acc):.4f}\")\n",
    "Write_log(log, f\">>  all_valid_metric : f1 = {f1_score(train_df.label, train_df.model_pred):.4f} ... acc = {accuracy_score(train_df.label, train_df.model_pred):.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment manage --\n",
    "mean_valid_metric = np.mean(fold_f1)\n",
    "all_valid_metric = f1_score(train_df.label, train_df.model_pred)\n",
    "\n",
    "log_df = pd.DataFrame(settings).T\n",
    "log_df[\"all_valid_metric\"] = all_valid_metric\n",
    "log_df[\"mean_valid_metric\"] = mean_valid_metric\n",
    "\n",
    "Write_exp_management(output_root, log_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
