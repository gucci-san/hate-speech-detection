{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習中断・再開の実装 --\n",
    "* train, validともにtrain.csv全データを使って、\n",
    "    * 一気に5epoch\n",
    "    * 1epochずつ5epoch <br>\n",
    "    行ったときの結果が一致するかを確認する --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "\n",
      "****** SEED fixed : 42 ******\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "from bert_utils import *\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_path+\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"clean_text\"] = train_df[\"text\"].map(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = define_tokenizer(\"cl-tohoku/bert-base-japanese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = HateSpeechDataset(train_df, tokenizer=tokenizer, max_length=76, num_classes=2, text_col=\"clean_text\", label_name=\"label\")\n",
    "valid_ds = HateSpeechDataset(train_df, tokenizer=tokenizer, max_length=76, num_classes=2, text_col=\"clean_text\", label_name=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=32, num_workers=2, shuffle=True, pin_memory=True, drop_last=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds, batch_size=64, num_workers=2, shuffle=False, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mconcatenate-4\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "model = HateSpeechModel(\"cl-tohoku/bert-base-japanese\", 2, custom_header=\"concatenate-4\", dropout=0.2, n_msd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** *** NOT implemented *** *** \n",
      "        --> CosineAnnealingLR *** *** \n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer and Scheduler --\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "scheduler = fetch_scheduler(optimizer=optimizer, scheduler=\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期状態から5epoch回す --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU : NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:10<00:00, 15.26it/s, Epoch=1, LR=7.6e-6, Train_Loss=0.216] \n",
      "100%|██████████| 83/83 [00:05<00:00, 15.33it/s, Epoch=1, LR=7.6e-6, Valid_Loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss Improved : inf ---> 0.112405\n",
      "Model Saved\n",
      "\n",
      "Training Complete in 0h 0m 27s\n",
      "Best Loss: 0.1124\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "model, history = run_training(\n",
    "    model, train_loader, valid_loader,\n",
    "    optimizer, scheduler, 1, device,\n",
    "    True, 5, \"_ALL\", \"./test/\",\n",
    "    log=None, save_checkpoint=True, load_checkpoint=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f007669f58917ef828e563fe3b1481c9ee4c6d5364b91c467fc73ebe5072978b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
